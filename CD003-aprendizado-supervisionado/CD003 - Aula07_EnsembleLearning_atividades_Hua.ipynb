{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Aula07_EnsembleLearning_atividades_Hua.ipynb","provenance":[{"file_id":"1_Md5ZukfBiR7cTmnmAPMncuO3kldE89j","timestamp":1653654439798},{"file_id":"1E4uX64ue8qdLr9-pPAnGRAyr-7R2vTMB","timestamp":1643573144875},{"file_id":"11KQaF5WWqgKOnoSWPy6kP3U46mZIkaPD","timestamp":1643397530200}],"collapsed_sections":[],"toc_visible":true,"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Especialização em Ciência de Dados - INF/UFRGS E SERPRO**\n","### Disciplina CD003 - Aprendizado Supervisionado\n","#### *Profa. Mariana Recamonde-Mendoza (mrmendoza@inf.ufrgs.br)*\n","<br> \n","\n","---\n","***Observação:*** *Este notebook é disponibilizado aos alunos como complemento às aulas síncronas e aos slides preparados pela professora. Desta forma, os principais conceitos são apresentados no material teórico fornecido. O objetivo deste notebook é reforçar os conceitos e demonstrar questões práticas no uso de diferentes algoritmos e estratégias de Aprendizado de Máquina.*\n","\n","\n","---\n","\n"],"metadata":{"id":"piZunEaUizIm"}},{"cell_type":"markdown","source":["<br>\n","\n","## **Aula 07** - **Tópico: Aprendizado Ensemble**\n","\n","<br>\n","\n","A teoria da **Sabedoria das multidões** estabelece que quando agregamos opiniões diversas e independentes na solução de um problema, temos grandes chances de encontrar uma solução que, no geral, é melhor do que as soluções individuais. Em Aprendizado de Máquina, esta teoria é explorada através do conceito de **aprendizado ensemble**, no qual múltiplos modelos são treinados para um mesmo conjunto de dados e suas saídas são agregadas para a tomada de decisão. Um aspecto chave no ensemble é a diversidade entre os modelos. Formas usuais de treinar um ensemble de modelos são: i) usar diferentes algoritmos e ii) usar dados de treinamento diferentes (amostras diferentes, por exemplo).\n","\n","O sklearn possui um [módulo](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) com diversos métodos de aprendizado ensemble para tarefas de classificação e regressão, alguns dos quais iremos utilizar neste notebook.\n","\n","**Objetivo deste notebook**: Explorar diferentes estratégias para construir *ensembles* de classificadores.\n","\n","\n","*Observação: O desenvolvimento deste notebook foi baseado no livro de Aurélien Géron, Hands-On Machine Learning with Scikit-Learn, Keras and Tensorflow (2nd Edition)*\n","\n","----\n"],"metadata":{"id":"JYZWwEfo3H1R"}},{"cell_type":"markdown","source":["###Carregando e dividindo os dados\n","\n","\n","Neste notebook, vamos utilizar novamente o dataset [Breast Cancer Wisconsin](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset), carregando-o através das funções do scikit-learn."],"metadata":{"id":"owi-J_whK4IS"}},{"cell_type":"code","source":["import pandas as pd             # biblioteca para análise de dados \n","import matplotlib.pyplot as plt # biblioteca para visualização de informações\n","import seaborn as sns           # biblioteca para visualização de informações\n","import numpy as np              # biblioteca para operações com arrays multidimensionais\n","from sklearn.datasets import load_breast_cancer ## conjunto de dados a ser analisado\n","sns.set()\n","\n","data = load_breast_cancer() ## carrega os dados de breast cancer\n","X = data.data  # matriz contendo os atributos\n","y = data.target  # vetor contendo a classe (0 para maligno e 1 para benigno) de cada instância\n","feature_names = data.feature_names  # nome de cada atributo\n","target_names = data.target_names  # nome de cada classe\n","\n","## Relembrando as características do dataset\n","print(f\"Dimensões de X: {X.shape}\\n\")\n","print(f\"Dimensões de y: {y.shape}\\n\")\n","print(f\"Nomes dos atributos: {feature_names}\\n\")\n","print(f\"Nomes das classes: {target_names}\")"],"metadata":{"id":"p-1VZTPjQax2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Iniciamos fazendo uma divisão dos dados em treino e teste na proporção 80%/20%. Fazemos de forma estratificada, isto é, mantendo a proporção original das classes ('stratify=y')."],"metadata":{"id":"sR11BS-7bYde"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split \n","\n","## Fazemos a divisão com 2-way holdout, de forma estratificada\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"],"metadata":{"id":"cIuIq4oSMVtk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fazemos a normalização dos dados, para evitar o impacto em algoritmos sensíveis a diferenças de escala entre atributos."],"metadata":{"id":"NJoxlKsSwuVO"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","scaler.fit(X_train)\n","X_train = scaler.transform(X_train)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"8VrqoQGCwtz6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bagging"],"metadata":{"id":"YTip3M8suNXe"}},{"cell_type":"markdown","source":["O método Bagging (Boostrap Aggregating) é baseado no treinamento de múltiplos modelos usando o mesmo algoritmo de aprendizado mas diferentes dados de treinamento obtidos através de amostragem com reposição. Embora seja possível realizar estas amostragem manualmente, o scikit-learn oferece um método que constrói este tipo de ensemble: [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier) . Este método tem como principais parâmetros:\n","\n","\n","*   base_estimator: o algoritmo usado para treinamento dos modelos individuais. Por padrão, utiliza o DecisionTreeClassifier.\n","*   n_estimators: o número de modelos do ensemble. Por padrão, utiliza 10.\n","*   bootstrap: define se a amostragem deve ser feita com reposição. Por padrão, tem valor True. \n","*   max_samples: o número de instâncias de cada amostra. Por padrão, é definida como o número de instâncias no conjunto de dados original.\n","\n"],"metadata":{"id":"CLv8uWfbubg-"}},{"cell_type":"code","source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","## Treina um classificador Bagging usando uma árvore de decisão como base\n","## São treinadas 501 árvores, cada qual com base em 50% dos dados de treinamento\n","bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=501, max_samples=0.5, n_jobs=-1, random_state=42)\n","bag_clf.fit(X_train, y_train)"],"metadata":{"id":"yRMv23LsvW5R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix,accuracy_score,ConfusionMatrixDisplay\n","\n","y_pred_bag = bag_clf.predict(X_test)\n","\n","cm = confusion_matrix(y_test, y_pred_bag,labels=bag_clf.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=bag_clf.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","\n","print(round(accuracy_score(y_test, y_pred_bag),3))"],"metadata":{"id":"hhmqKMW5weEW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["O sklearn possui uma versão de Bagging para regressão: [BaggingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor)\n"],"metadata":{"id":"vBjTt6fu2TM_"}},{"cell_type":"markdown","source":["### Random Forests"],"metadata":{"id":"ecBPycB602yi"}},{"cell_type":"markdown","source":["Como vimos, o Random Forests (Florestas Aleatórias) é um algoritmo muito semelhante ao Bagging. A diferença é que ele introduz uma aleatoriedade extra ao construir as árvores de decisão: ao invés de escolher o melhor dentre todos os atributos para uma nova divisão de nós, o algoritmo escolhe o melhor dentre um subconjunto aleatório de atributos. No sklearn, este tipo de ensemble é implementado no método [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier). Além dos hiperparâmetros comuns a árvores de decisão (como criterion, max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes, etc.), este método tem como principais parâmetros:\n","\n","*   n_estimators: o número de modelos do ensemble. Por padrão, utiliza 100.\n","*   bootstrap: define se a amostragem deve ser feita com reposição. Por padrão, tem valor True. \n","*   max_samples: o número de instâncias de cada amostra. Por padrão, é definida como o número de instâncias no conjunto de dados original.\n","\n"],"metadata":{"id":"_qIz13tG07eJ"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","## Treina um classificador Random Forests.\n","## São treinadas 501 árvores, usando pré-poda (profundidade máxima=5)\n","rf_clf = RandomForestClassifier(n_estimators=501, max_depth=5, random_state=42)\n","rf_clf.fit(X_train, y_train)"],"metadata":{"id":"3vvRpAnw2l2Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","y_pred_rf = rf_clf.predict(X_test)\n","\n","cm = confusion_matrix(y_test, y_pred_rf,labels=rf_clf.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_clf.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","\n","print(round(accuracy_score(y_test, y_pred_rf),3))"],"metadata":{"id":"qxMq5b1I2nry"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["O sklearn possui uma versão de Random Forests para regressão: [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)\n"],"metadata":{"id":"tK_mWrY0_Fgk"}},{"cell_type":"markdown","source":["### Árvores de Decisão vs Bagging vs Random Forests"],"metadata":{"id":"zEBBe37-5jAV"}},{"cell_type":"markdown","source":["Apenas para fins de visualização, visando observar como os diferentes métodos baseados em Bagging podem encontrar fronteiras de decisão mais complexas que uma árvore de decisão (algoritmo usando como 'base' no ensemble), vamos usar um conjunto de dados sintético para treinar modelos baseados nos três algoritmos."],"metadata":{"id":"jpxAVms35oUI"}},{"cell_type":"code","source":["from sklearn.datasets import make_moons\n","X_moon, y_moon = make_moons(n_samples=500, noise=0.30, random_state=42)\n","X_moon_train, X_moon_test, y_moon_train, y_moon_test = train_test_split(X_moon, y_moon, random_state=42,test_size=0.25)\n"],"metadata":{"id":"QPyzSGfD5GIJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tree_moon = DecisionTreeClassifier(random_state=42)\n","tree_moon.fit(X_moon_train, y_moon_train)\n","\n","bag_moon = BaggingClassifier(DecisionTreeClassifier(), n_estimators=501,\n","                            max_samples=100, n_jobs=-1, random_state=42)\n","bag_moon.fit(X_moon_train, y_moon_train)\n","\n","\n","rf_moon = RandomForestClassifier(n_estimators=501, max_depth=5, random_state=42)\n","rf_moon.fit(X_moon_train, y_moon_train)\n","\n","\n","## Função para plotar as fronteiras de decisão\n","def plot_decision_boundary(clf, X, y, alpha=1.0):\n","    axes=[-1.5, 2.4, -1, 1.5]\n","    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n","                         np.linspace(axes[2], axes[3], 100))\n","    X_new = np.c_[x1.ravel(), x2.ravel()]\n","    y_pred = clf.predict(X_new).reshape(x1.shape)\n","    \n","    plt.contourf(x1, x2, y_pred, alpha=0.3 * alpha, cmap='Wistia')\n","    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8 * alpha)\n","    colors = [\"#78785c\", \"#c47b27\"]\n","    markers = (\"o\", \"^\")\n","    for idx in (0, 1):\n","        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],\n","                 color=colors[idx], marker=markers[idx], linestyle=\"none\")\n","    plt.axis(axes)\n","    plt.xlabel(r\"$x_1$\")\n","    plt.ylabel(r\"$x_2$\", rotation=0)\n","\n","## Chamanda a função para cada modelo, e gerando a figura\n","fig, axes = plt.subplots(ncols=3, figsize=(18, 6), sharey=True)\n","plt.sca(axes[0])\n","plot_decision_boundary(tree_moon,X_moon_train, y_moon_train)\n","plt.title(\"Decision Tree\")\n","plt.sca(axes[1])\n","plot_decision_boundary(bag_moon,X_moon_train, y_moon_train)\n","plt.title(\"Decision Trees with Bagging\")\n","plt.sca(axes[2])\n","plot_decision_boundary(rf_moon,X_moon_train, y_moon_train)\n","plt.title(\"Random Forests\")\n","plt.ylabel(\"\")\n","#save_fig(\"decision_tree_vs_ensembles\")\n","plt.show()"],"metadata":{"id":"gA8zvppd6B2f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Boosting - Adaboost"],"metadata":{"id":"4inwjHf04Tf3"}},{"cell_type":"markdown","source":["Boosting são métodos ensemble capazes de combinar modelos fracos (isto é, que possuem um alto viés) em um modelo mais forte. A ideia é treinar preditores de forma sequencial de modo que cada preditor tenta se especializar (e corrigir) nos erros do classificador anterior. Embora existam muitos algoritmos de aprendizado ensemble que exploram a proposta de Boosting, o mais comum é o Adaboost. Neste algoritmo, cada instância possui um peso associado, os quais inicialmente são uniformes. A cada classificador treinado, o peso das instâncias classificadas corretamente é diminuído, enquanto o peso das instâncias classificadas incorretamente é aumentado. Mais informações podem ser obtidas na documentação do método [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier). Os principais parâmetros deste método são:\n","\n","*   base_estimator: o algoritmo usado para treinamento dos modelos individuais. Por padrão, utiliza o DecisionTreeClassifier com max_depth=1 (alto viés).\n","*   n_estimators: o número de modelos do ensemble. Por padrão, utiliza 50.\n","*   learning_rate: o peso atribuído a cada classificador em cada iteração do Boosting. Quanto maior o valor, mais aumenta a contribuição de cada classificador. Por padrão, utiliza learning_rate igual a 1. \n"],"metadata":{"id":"4KK7FlQl4ZPH"}},{"cell_type":"code","source":["from sklearn.ensemble import AdaBoostClassifier\n","\n","## Treina um classificador AdaBoost, com 100 modelos criados de forma sequencial\n","## Cada modelo é uma árvore de decisão com max_depth=1\n","adab_clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n","adab_clf.fit(X_train, y_train)"],"metadata":{"id":"JGnpnfL17o66"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred_adab = adab_clf.predict(X_test)\n","\n","cm = confusion_matrix(y_test, y_pred_adab,labels=adab_clf.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=adab_clf.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","\n","print(round(accuracy_score(y_test, y_pred_adab),3))"],"metadata":{"id":"xNjNZTno7pVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["O sklearn possui uma versão de AdaBoost para regressão: [AdaBoostRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor)\n"],"metadata":{"id":"0sxMlPFw_XPW"}},{"cell_type":"markdown","source":["### Voting Classifier"],"metadata":{"id":"JlyOzZHF9V0o"}},{"cell_type":"markdown","source":["Uma das formas mais simples de criar um modelo ensemble utilizando diferentes algoritmos de aprendizado de máquina é através do método de votação. O sklearn possui um método que facilita a aplicação desta estratégia: [VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier). Os principais parâmetros do método são:\n","\n","\n","*   estimators: lista dos algoritmos a serem usados como base do ensemble\n","*   voting: 'hard' para aplicar uma votação majoritária (baseada nos rótulos preditos), ou 'soft' para retornar como saída do ensemble a classe que maximiza a soma das probabilidades preditas por todos os modelos. Por padrão, utiliza 'hard'.\n","*   weights: pesos a serem atribuídos a cada classificador. Por padrão, utiliza pesos uniformes.\n","\n","\n","\n","Vamos exemplificar o seu uso a seguir."],"metadata":{"id":"9MmyDlwNPtYw"}},{"cell_type":"code","source":["from sklearn.ensemble import VotingClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","\n","voting_clf = VotingClassifier(\n","    estimators=[\n","        ('lr', LogisticRegression(random_state=42)),\n","        ('rf', RandomForestClassifier(random_state=42)),\n","        ('svc', SVC(random_state=42))\n","    ]\n",")\n","voting_clf.fit(X_train, y_train)"],"metadata":{"id":"UTybEpRX-Ra8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred_voting = voting_clf.predict(X_test)\n","\n","cm = confusion_matrix(y_test, y_pred_voting,labels=voting_clf.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=voting_clf.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","\n","print(round(accuracy_score(y_test, y_pred_voting),3))"],"metadata":{"id":"TCTWM4bX_kAe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Stacking"],"metadata":{"id":"HThzjgcP_sYp"}},{"cell_type":"markdown","source":["Outra estratégia para a agregação de modelos treinados com diferentes métodos de aprendizado a partir dos mesmos dados é o Stacking. A ideia é simples: após o treinamento dos classificadores individuais (as 'bases' do ensemble), um modelo é treinado para combinar as saídas deles em uma única decisão. Esta estratégia está implementada no método [StackingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier). Os principais parâmetros do método são:\n","\n","*   estimators: lista dos algoritmos a serem usados como base do ensemble\n","*   final_estimator: algoritmo a ser usado para combinar a saída dos classificadores bases. Por padrão, usa LogisticRegression\n","*   cv: determina a configuração da validação cruzada (método usado para treinamento e validação de modelos) a ser utilizada para treinar o final_estimator. Pode ser um valor inteiro que define número de folds, ou 'prefit' quando os classificadores base já estão treinados. Por padrão, usa 5-fold cross-validation\n","\n"],"metadata":{"id":"hwrfEUPW_uoH"}},{"cell_type":"code","source":["from sklearn.ensemble import StackingClassifier\n","\n","stacking_clf = StackingClassifier(\n","    estimators=[\n","        ('lr', LogisticRegression(random_state=42)),\n","        ('rf', RandomForestClassifier(random_state=42)),\n","        ('svc', SVC(probability=True, random_state=42))\n","    ],\n","    final_estimator=RandomForestClassifier(random_state=43),\n","    cv=5  # number of cross-validation folds\n",")\n","stacking_clf.fit(X_train, y_train)"],"metadata":{"id":"hJ8hM5xh_t1A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred_stacking = stacking_clf.predict(X_test)\n","\n","cm = confusion_matrix(y_test, y_pred_stacking,labels=stacking_clf.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=stacking_clf.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","\n","print(round(accuracy_score(y_test, y_pred_stacking),3))"],"metadata":{"id":"u-j4E1TeBBZx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["O sklearn possui uma versão de VotingClassifier para regressão: [VotingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html#sklearn.ensemble.VotingRegressor)\n"],"metadata":{"id":"6V0rxdDF_HcY"}}]}