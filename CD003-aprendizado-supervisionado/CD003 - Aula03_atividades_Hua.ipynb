{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Aula03_atividades_Hua.ipynb","provenance":[{"file_id":"1OP4QQmp-sYF9IcoEtIahij2kPnLigDMR","timestamp":1652374944218}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Especialização em Ciência de Dados - INF/UFRGS e SERPRO**\n","### Disciplina CD003 - Aprendizado Supervisionado\n","#### *Profa. Mariana Recamonde-Mendoza (mrmendoza@inf.ufrgs.br)*\n","<br> \n","\n","---\n","***Observação:*** *Este notebook é disponibilizado aos alunos como complemento às aulas síncronas e aos slides preparados pela professora. Desta forma, os principais conceitos são apresentados no material teórico fornecido. O objetivo deste notebook é reforçar os conceitos e demonstrar questões práticas no uso de diferentes algoritmos e estratégias de Aprendizado de Máquina.*\n","\n","\n","---\n","\n"],"metadata":{"id":"0FA12O8Tpjmk"}},{"cell_type":"markdown","source":["<br>\n","\n","## **Aula 03** - **Tópico: Algoritmo Naive Bayes**\n","\n","<br>\n","\n","O algoritmo **Naive Bayes** é um algoritmo probabilístico que assume que os valores dos atributos de uma instância são independentes entre si dada a classe. Assim, ao determinar a probabilidade a posteriori $P(y_i|\\textbf{x})$ para uma determinada classe $i$ e uma instância $\\textbf{x}$ através do Teorema de Bayes, o algoritmo calcula $P(\\textbf{x}|y_i)$ como o produto das probabilidades condicionais de cada atributo individualmente ($x^i, x^2, ..., x^d$) considerando a classe $i$.\n","\n","A simplicidade na implementação do algoritmo, o fato de representar o seu conhecimento através de probabilidades que são compreensíveis por humanos, a ausência de (muitos) hiperparâmetros que demandam ajustes, e a sua robustez a ruídos, tornam o **Naive Bayes** um algoritmo bastante interesse para domínios que apresentam incerteza e como uma primeira escolha de algoritmo para abordar novos problemas de classificação. Além disso, é um algoritmo que tende a se adaptar melhor em problemas com alta dimensionalidade (número de atributos >> número de instâncias).\n","<br> \n","\n","**Objetivo deste notebook**: Compreender o uso do algoritmo Naive Bayes em problemas de classificação e analisar soluções para lidar com conjuntos de dados que apresentam tipos de atributos mistos (numéricos e categóricos). \n","\n","<br>\n","\n","---\n","\n"],"metadata":{"id":"sb8YuHnP2u2s"}},{"cell_type":"markdown","source":["\n","\n","##**Analisando o risco de doenças cardíacas**\n","\n","De acordo com o Centers for Disease Control and Prevention (CDC) dos Estados Unidos, a doença cardíaca é uma das principais causas de morte para pessoas da maioria das raças nos EUA (afro-americanos, índios americanos e nativos do Alasca e pessoas brancas). Cerca de metade de todos os americanos (47%) têm pelo menos 1 dos 3 principais fatores de risco para doenças cardíacas: pressão alta, colesterol alto e tabagismo. Outros indicadores-chave incluem estado diabético, obesidade (IMC alto), não praticar atividade física suficiente ou beber muito álcool. Detectar e prevenir os fatores que têm maior impacto nas doenças cardíacas é muito importante na área da saúde. \n","\n","Os dados a serem analisados neste notebook foram disponibilizados no Kaggle e derivam do *Behavioral Risk Factor Surveillance System (BRFSS)*, que realiza pesquisas telefônicas anuais para coletar dados sobre o estado de saúde dos residentes dos EUA. Como o CDC descreve: *\"Estabelecido em 1984 com 15 estados, o BRFSS agora coleta dados em todos os 50 estados, bem como no Distrito de Columbia e três territórios dos EUA. O BRFSS completa mais de 400.000 entrevistas com adultos a cada ano, tornando-se o maior sistema de pesquisa no mundo.\"*. O conjunto de dados mais recente (em 15 de fevereiro de 2022) inclui dados de 2020. A grande maioria das colunas são perguntas feitas aos entrevistados sobre seu estado de saúde, como \"Você tem sérias dificuldades para andar ou subir escadas?\" ou \"Você fumou pelo menos 100 cigarros em toda a sua vida? [Nota: 5 maços = 100 cigarros]\". A grande base de dados gerada pela pesquisa realizada pelo CDC foi filtrada, de forma a manter fatores diferentes (perguntas) que influenciam direta ou indiretamente as doenças cardíacas.\n","\n","Nesta atividade, iremos utilizar o algoritmo de Naive Bayes para prever a chance de uma pessoa ter doença cardíaca a partir de um conjunto de perguntas que se relacionam a fatores de risco da doença. Os dados a serem utilizados foram filtrados pela professora, a fim de manter um subconjunto de **60 mil** instâncias.\n"],"metadata":{"id":"DbJNQfaM4ERT"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"jiutqCcGYYM0"}},{"cell_type":"markdown","source":["###Carregando e inspecionando os dados\n","\n","Primeiramente, vamos carregar algumas bibliotecas importantes do Python e os dados a serem utilizados neste estudo. Os dados são disponibilizados através de um link, que também pode ser diretamente acessado pelos alunos."],"metadata":{"id":"dbr-VwMq6OPG"}},{"cell_type":"code","source":["## Carregando as bibliotecas necessárias\n","# A primeira linha é incluída para gerar os gráficos logo abaixo dos comandos de plot\n","%matplotlib inline              \n","import pandas as pd             # para análise de dados \n","import matplotlib.pyplot as plt # para visualização de informações\n","import seaborn as sns           # para visualização de informações\n","import numpy as np              # para operações com arrays multidimensionais\n","from sklearn.naive_bayes import GaussianNB # para treinar NB com dados contínuos\n","from sklearn.naive_bayes import CategoricalNB # para treinar NB com dados categóricos\n","from sklearn.model_selection import train_test_split # para divisão de dados\n","from sklearn.metrics import confusion_matrix, recall_score, precision_score,accuracy_score,ConfusionMatrixDisplay ## para avaliação dos modelos\n","\n","\n","sns.set()\n"],"metadata":{"id":"2lyNg5Sluhbg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"https://drive.google.com/uc?export=view&id=1WOW3Q3IL6bTbrNkyP1YhdMbsU31I_R2g\")\n","df  "],"metadata":{"id":"bPcnFsAB9kv0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Características gerais do dataset\n","print(\"O conjunto de dados possui {} linhas e {} colunas\".format(df.shape[0], df.shape[1]))"],"metadata":{"id":"iCBVXyya-_tM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A coluna *'HeartDisease'* contém a classificação de cada amostra referente a ocorrência de doença cardíaca. Vamos avaliar como as instâncias estão distribuídas entre as classes presentes no dataset."],"metadata":{"id":"UqoChZTpAEMu"}},{"cell_type":"code","source":["## Distribuição do atributo alvo\n","plt.hist(df['HeartDisease'])\n","plt.title(\"Distribuição do atributo alvo\")\n","plt.show()"],"metadata":{"id":"h0NT9TCiAeLX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Também é importante averiguar os tipos de dados de cada coluna (atributo), bem como se existem valores faltantes. Os valores faltantes normalmente estão codificados como NaN e serão identificados pelo Python com o comando `isnull()`. Caso estejam codificados de outra forma, é necessário substituir por NaN."],"metadata":{"id":"9FfabKeQT8ja"}},{"cell_type":"code","source":["df.info()\n","\n","## Para analisar valores faltantes, quando codificados como NaN, podemos usar o\n","## comando abaixo\n","df.isnull().sum()"],"metadata":{"id":"yA30OBnnqPXf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Podemos perceber que existem atributos do tipo categórico (object) e do tipo numérico (int64). Vamos analisar cada um dos tipos, bem como dividir em variáveis distintas para facilitar a manipulação dos dados."],"metadata":{"id":"JMM55kP1URmk"}},{"cell_type":"code","source":["## Encontrar as variáveis categóricas\n","categorical = [var for var in df.columns if df[var].dtype=='O']\n","\n","## Heart disease é categórica, mas se trata do atributo alvo - removemos da lista\n","categorical = categorical [1:len(categorical)]\n","\n","print('Existem {} atributos categóricos no conjunto de dados\\n'.format(len(categorical)))\n","\n","print('Os atributos categóricos são :', categorical)"],"metadata":{"id":"8Lwu9AcroLlg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para os atributos categóricos, podemos investigar quantos valores diferentes cada um deles pode assumir, bem como a distribuição das classes entre as categorias de cada atributo."],"metadata":{"id":"HKkEtNChUrn-"}},{"cell_type":"code","source":["##Verificar a cardinalidade dos valores categóricos \n","for var in categorical:\n","    print(var, ' contains ', len(df[var].unique()), ' labels')"],"metadata":{"id":"xcdb6q4Ls19g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Gerar um gráfico para cada variável categórica com a distribuição de \n","## frequência entre as classes\n","def count_plot(df,columns,label):\n","    plt.figure(figsize=(16, 10))\n","    for indx, var  in enumerate(columns):\n","        plt.subplot(4, 4, indx+1)\n","        g = sns.countplot(x=var, data=df, hue=label)\n","    plt.tight_layout()\n","\n","count_plot(df, categorical,'HeartDisease')"],"metadata":{"id":"zdoptZ4EtGIr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos olhar com mais atenção os atributos AgeCategory, Race e Diabetic, pois possuem mais categorias que não puderam ser bem visualizadas no gráfico acima. A célula abaixo executa para o primeiro atributo. Modifique e analise para os demais atributos mencionados."],"metadata":{"id":"FL4ksZY7VEZo"}},{"cell_type":"code","source":["plt.figure(figsize=(16, 10))\n","\n","#sns.countplot(x='AgeCategory', data=df, hue='HeartDisease')\n","#sns.countplot(x='Race', data=df, hue='HeartDisease')\n","sns.countplot(x='Diabetic', data=df, hue='HeartDisease')\n","\n"],"metadata":{"id":"VkLGRX-kVKeP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Responda >>>** Observe a frequência de HeartDisease para cada variável categórica. Alguma variável parece ter um desequilíbrio na frequência das classes para os diferentes valores que pode assumir?"],"metadata":{"id":"QopftIi4FNm7"}},{"cell_type":"markdown","source":["> ***Sua resposta aqui:*** No gráfico por Race, há muito mais ocorrências para brancos (White) do que não brancos, e também não parece haver ocorrências nos dados para o caso asiático (Asian) com doença cardíaca (HeartDisease = Yes). No gráfico por Diabetic, há poucas ocorrências paras os casos de pré-diabetes (borderline diabetes) e durante a gravidez (during pregnancy)"],"metadata":{"id":"J8l4kYxHrwwW"}},{"cell_type":"markdown","source":["Vamos repetir a análise agora para os atributos numéricos. O código abaixo identifica as variáveis numéricas e plota as suas distribuições de valores."],"metadata":{"id":"57_WB7PqW5_A"}},{"cell_type":"code","source":["## Encontrar as variáveis numéricas\n","\n","numerical = [var for var in df.columns if df[var].dtype!='O']\n","\n","print('Existem {} atributos numéricos no conjunto de dados\\n'.format(len(numerical)))\n","\n","print('Os atributos numéricos são :', numerical)"],"metadata":{"id":"WC6wXmhDp1rX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Gerar um gráfico para cada variável numérica com a distribuição de \n","## frequência entre as classes\n","def dist_plot(df,columns,label):\n","    plt.figure(figsize=(16, 10))\n","    for indx, var  in enumerate(columns):\n","        plt.subplot(2, 2, indx+1)\n","        g = sns.histplot(x=var, data=df, hue=label,binwidth=3)\n","    plt.tight_layout()\n","\n","dist_plot(df, numerical,'HeartDisease')"],"metadata":{"id":"7QEWJ4FdEtxa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","\n","\n","### Criando conjuntos de treino e teste para avaliação de modelos\n","\n","\n","Antes de iniciar o treinamento do modelo, vamos aplicar o método holdout e separar uma porção dos dados para teste.\n","\n","O algoritmo Naive Bayes tem várias implementações no scikit-learn. Vamos adotar duas delas ao longo da atividade: CategoricalNB e GaussianNB, onde a primeira aceita como entrada somente dados categóricos e a segunda somente dados numéricos. Um dos pré-requisitos para uso da CategoricalNB() é que os dados categóricos estejam codificados em valores numéricos (ao invés de [Baixo, Médio, Alto], representar os valores por [0, 1, 2], por exemplo). Assim, vamos aplicar esta codificação e então dividir os dados em conjuntos de treino (80%) e teste (20%).\n","\n"],"metadata":{"id":"63nQ-E0jVQzW"}},{"cell_type":"code","source":["from sklearn.preprocessing import OrdinalEncoder # para converter variáveis categóricas (strings para inteiros)\n","\n","## Separa o dataset em duas variáveis: os atributos/entradas (X) e a classe/saída (y)\n","## Originalmente a classe se encontra na primeira coluna ('HeartDisease')\n","X = df.drop(['HeartDisease'], axis=1)\n","y = df['HeartDisease'].values\n","\n","# Codifica variáveis categóricas usando inteiros, pré-requisito para CategoricalNB()\n","encoder = OrdinalEncoder(dtype=np.int64)\n","encoder.fit(X[categorical])\n","X[categorical] = encoder.transform(X[categorical])\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,stratify=y,random_state=42) ## fixe um número de random_state para facilitar a reprodutibilidade"],"metadata":{"id":"6Z-BgAqfoc8i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","### Treinamento de modelos individuais para dados categóricos e numéricos\n","\n","Vamos fazer agora dois treinamentos distintos, primeiro um modelo com base nos atributos categóricos e em seguida um modelo com base nos atributos numéricos.\n","\n","Iniciamos separando os conjuntos de dados para os dois tipos de atributos."],"metadata":{"id":"zuhzlay5bmQB"}},{"cell_type":"code","source":["## Identificamos os índices das colunas para cada tipo de atributo\n","catFeat_idx = np.argwhere(X.columns.isin(categorical)).ravel()\n","numFeat_idx = np.argwhere(X.columns.isin(numerical)).ravel()\n","\n","## A partir dos índices, separamos os atributos usados no treino e no teste\n","## A classe (y) não precisa ser separada, pois é a mesma para ambos\n","X_train_cat = X_train.iloc[:,catFeat_idx]\n","X_train_num = X_train.iloc[:,numFeat_idx]\n","\n","X_test_cat = X_test.iloc[: ,catFeat_idx]\n","X_test_num = X_test.iloc[: ,numFeat_idx]"],"metadata":{"id":"ID28dkOFXDqR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Abaixo definimos uma funçao auxiliar para visualizar a fronteira de decisão dos classificadores, semelhante à que foi usada na atividade anterior."],"metadata":{"id":"bHRQDR0Okdkq"}},{"cell_type":"code","source":["## Fonte: https://github.com/tirthajyoti/Machine-Learning-with-Python/blob/master/Utilities/ML-Python-utils.py\n","def plot_decision_boundaries(X, y, model_class, **model_params):\n","    \"\"\"\n","    Function to plot the decision boundaries of a classification model.\n","    This uses just the first two columns of the data for fitting \n","    the model as we need to find the predicted value for every point in \n","    scatter plot.\n","    Arguments:\n","            X: Feature data as a NumPy-type array.\n","            y: Label data as a NumPy-type array.\n","            model_class: A Scikit-learn ML estimator class \n","            e.g. GaussianNB (imported from sklearn.naive_bayes) or\n","            LogisticRegression (imported from sklearn.linear_model)\n","            **model_params: Model parameters to be passed on to the ML estimator\n","    \n","    Typical code example:\n","            plt.figure()\n","            plt.title(\"KNN decision boundary with neighbros: 5\",fontsize=16)\n","            plot_decision_boundaries(X_train,y_train,KNeighborsClassifier,n_neighbors=5)\n","            plt.show()\n","    \"\"\"\n","    try:\n","        X = np.array(X)\n","        y = np.array(y).flatten()\n","    except:\n","        print(\"Coercing input data to NumPy arrays failed\")\n","\n","    # Reduces to the first two columns of data - for a 2D plot!\n","    reduced_data = X[:, :2]\n","    # Instantiate the model object\n","    model = model_class(**model_params)\n","    # Fits the model with the reduced data\n","    model.fit(reduced_data, y)\n","\n","    # Step size of the mesh. Decrease to increase the quality of the VQ.\n","    h = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].    \n","\n","    # Plot the decision boundary. For that, we will assign a color to each\n","    x_min, x_max = reduced_data[:, 0].min() - 0.5, reduced_data[:, 0].max() + 0.5\n","    y_min, y_max = reduced_data[:, 1].min() - 0.5, reduced_data[:, 1].max() + 0.5\n","    # Meshgrid creation\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","\n","    # Obtain labels for each point in mesh using the model.\n","    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])    \n","\n","    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n","    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n","                         np.arange(y_min, y_max, 0.01))\n","\n","    # Predictions to obtain the classification results\n","    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n","\n","    # Plotting\n","    plt.contourf(xx, yy, Z, alpha=0.2,cmap='viridis')\n","    g=plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.6,s=50, edgecolor='k', cmap='viridis' )\n","    plt.xlabel(\"Feature-1\",fontsize=15)\n","    plt.ylabel(\"Feature-2\",fontsize=15)\n","    plt.xticks(fontsize=14)\n","    plt.yticks(fontsize=14)\n","    plt.legend(handles=g.legend_elements()[0],labels=('No','Yes'))\n","    return plt"],"metadata":{"id":"-L0Xl1p9kiHC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Modelo Naive Bayes com dados numéricos\n","\n","Iniciamos criando um modelo Naive Bayes para os dados numéricos, usando o método `GaussianNB()`. Este modelo usa apenas 4 atributos, para lembrarmos:"],"metadata":{"id":"_ROVg6DHcTkM"}},{"cell_type":"code","source":["print(numerical)"],"metadata":{"id":"Va-ZAk30coyz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Treinando o modelo com GaussianNB\n","nbG_clf = GaussianNB()\n","nbG_clf.fit(X_train_num,y_train)\n","print(nbG_clf.class_prior_)\n","\n","## Prevendo a classe de saída para as instâncias de teste\n","## Por padrão, a classe retornada é aquela que maximiza a probabilidade a posteriori\n","y_predG = nbG_clf.predict(X_test_num)\n","\n","## As probabilidades por classe podem ser observadas com o seguinte comando\n","y_probaG = nbG_clf.predict_proba(X_test_num)\n","print(y_probaG[:10])"],"metadata":{"id":"1nummYZNIAb9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Avaliando o desempenho do modelo Naive Bayes Gaussiano."],"metadata":{"id":"Y_Vvi5dofNbh"}},{"cell_type":"code","source":["cm = confusion_matrix(y_test, y_predG,labels=nbG_clf.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nbG_clf.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","\n","print('Acurácia: {}'.format(round(accuracy_score(y_test, y_predG),3)))\n","print('Recall: {}'.format(round(recall_score(y_test, y_predG,pos_label='Yes'),3)))\n","print('Precisão: {}'.format(round(precision_score(y_test, y_predG,pos_label='Yes'),3)))"],"metadata":{"id":"T1fHFd8KfM54"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#####Observando a fronteira de decisão do modelo NB Gaussiano\n"],"metadata":{"id":"OA607j-SkqcD"}},{"cell_type":"code","source":["NUM_INSTANCES = 500\n","y_train_int =  np.array([0 if y=='No' else 1 for y in y_train]) \n","\n","plot_decision_boundaries(X_train_num[:NUM_INSTANCES],y_train_int[:NUM_INSTANCES],GaussianNB)"],"metadata":{"id":"xKrAzUYfkqEl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**>>>> Responda:** Como a fronteira de decisão muda para diferentes tamanhos de subconjuntos de instâncias? Altere o valor de NUM_INSTANCES, tentando valores menores e maiores do que o valor inicial.\n","\n","***Sua resposta aqui:*** Testei valores 40, 30, 20 e 10 abaixo de 50, e valores 60, 70, 90, 100, 150, 200, acima de 50.\n","\n","Foi possível perceber a linha ficar cada vez menos curva e se assemalhar com um reta para valores muito altos."],"metadata":{"id":"9kfgytB2kwGd"}},{"cell_type":"markdown","source":["#### Modelo Naive Bayes com dados categóricos\n","\n","Agora vamos treinar um modelo Naive Bayes para os dados categóricos, usando o método `CategoricalNB()`."],"metadata":{"id":"5fqgUqMAjvwk"}},{"cell_type":"code","source":["## Treinando o modelo com CategoricalNB\n","nbC_clf = CategoricalNB(alpha=1.0e-10)\n","nbC_clf.fit(X_train_cat,y_train)\n","\n","print(nbC_clf.class_log_prior_)\n","\n","## Prevendo a classe de saída para as instâncias de teste\n","## Por padrão, a classe retornada é aquela que maximiza a probabilidade a posteriori\n","y_predC = nbC_clf.predict(X_test_cat)\n","\n","## As probabilidades por classe podem ser observadas com o seguinte comando\n","y_probaC = nbC_clf.predict_proba(X_test_cat)\n","print(y_probaC[:10])\n","\n"],"metadata":{"id":"IOf6mSbI8p8R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Avaliando o desempenho do modelo Naive Bayes Categórico"],"metadata":{"id":"05A3FgFEj9SC"}},{"cell_type":"code","source":["cm = confusion_matrix(y_test, y_predC,labels=nbC_clf.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nbC_clf.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","\n","print('Acurácia: {}'.format(round(accuracy_score(y_test, y_predC),3)))\n","print('Recall: {}'.format(round(recall_score(y_test, y_predC,pos_label='Yes'),3)))\n","print('Precisão: {}'.format(round(precision_score(y_test, y_predC,pos_label='Yes'),3)))"],"metadata":{"id":"CmXQYxA2f6Tr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**>>>> Exercício:** Como se comparam os desempenhos dos dois modelos em relação às métricas análisadas e ao número de FP, FN, TP, TN?\n","\n","***Sua resposta aqui:*** \n","\n","Naive Bayes Gaussiano, apresentou um número muito elevado de falso positivo, um número muito acima dos verdadeiros postivos.\n","\n","Em relação aos valores de acurácia, recall e precisão o algoritimo Naive Bayes Categórico apresentou resultados melhores."],"metadata":{"id":"PPUjSepRkNNV"}},{"cell_type":"markdown","source":["#####Observando a fronteira de decisão do modelo NB Categórico"],"metadata":{"id":"uB_brWlJlW0D"}},{"cell_type":"code","source":["NUM_INSTANCES = 500\n","y_train_int =  np.array([0 if y=='No' else 1 for y in y_train]) \n","\n","plot_decision_boundaries(X_train_cat.iloc[:NUM_INSTANCES,:],y_train_int[:NUM_INSTANCES],CategoricalNB)"],"metadata":{"id":"dbt_WBpjKfWJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","### Naive Bayes com dados Categóricos e Numéricos - Abordagem 1\n","\n","Uma abordagem possível para utilizar dados de atributos mistos (categóricos e numéricos) é realizar o treinamento separadamente e agregar as probabilidades de cada modelo para tomar a decisão final através da multiplicação.\n","\n","Neste caso, é importante notar que as probabilidades retornadas pela função `predict_proba()` já incluem a multiplicação pela probabilidade a priori. Ao agregarmos esta probabilidade, é preciso corrigir a duplicidade no uso da probabilidade a priori das classes. Assim, seguimos os seguintes passos\n","\n","\n","1.   Treinamos cada modelo separadamente (exercício anterior)\n","2.   Obtemos as probabilidades preditas para cada instância de teste com cada modelo, obtendo y_probaC e y_probaG.\n","3.   Multiplicamos as probabilidades obtidas para cada classificador, para cada instância/classe\n","4.    Corrigimos a multiplicação, dividindo pela probabilidade a priori das classes\n","5.    Normalizamos os resultados obtidos (para cada instância a soma deve ser 1)\n","\n","\n","\n","Para esta abordagem, vamos utilizar os modelos individuais previamente treinados e suas respectivas probabilidades calculadas para os exemplos de teste."],"metadata":{"id":"bCJKgrmii86K"}},{"cell_type":"code","source":["from sklearn.preprocessing import normalize\n","\n","## O que vamos calcular:\n","## ProbGaussian * (ProbCategorical/PriorProb)\n","## Normalizando os resultados para que a probabilidade para cada instância some 1 (norm=l1_)\n","normed_matrix = normalize(y_probaC*(y_probaG/nbG_clf.class_prior_), axis=1, norm='l1')\n","#print(np.sum(normed_matrix,axis=1))\n","\n","## Analisamos a coluna (0 ou 1) com valor máximo\n","max_index_col = np.argmax(normed_matrix, axis=1).tolist()\n","\n","## Mapeamos os valores inteiros (0 ou 1) para o nome das classes\n","y_predM2 = np.array(['No' if y==0 else 'Yes' for y in max_index_col]) \n","\n","cm = confusion_matrix(y_test, y_predM2,labels=nbC_clf.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nbC_clf.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","\n","print('Acurácia: {}'.format(round(accuracy_score(y_test, y_predM2),3)))\n","print('Recall: {}'.format(round(recall_score(y_test, y_predM2,pos_label='Yes'),3)))\n","print('Precisão: {}'.format(round(precision_score(y_test, y_predM2,pos_label='Yes'),3)))"],"metadata":{"id":"AMKwK6LqfdkN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","### Naive Bayes com dados Categóricos e Numéricos - Abordagem 2\n","\n","Outra abordagem possível para treinar um modelo Naive Bayes usando dados mistos (categóricos e numéricos) é discretizar os atributos numéricos usando um processo de \"*binning*\". Vamos usar o conceito de percentil para discretizar os dados numéricos. Para o atributo BMI (IMC, índice de massa corporal), usaremos os pontos de cortes estabelecidos na interpretação deste índice."],"metadata":{"id":"sZzzglyze6Vm"}},{"cell_type":"code","source":["## Separa o dataset em duas variáveis: os atributos/entradas (X) e a classe/saída (y)\n","## Originalmente a classe se encontra na primeira coluna ('HeartDisease')\n","X = df.drop(['HeartDisease'], axis=1)\n","y = df['HeartDisease'].values\n","\n","## Discretizar variáveis contínuas, divindindo a distribuição em 5 bins (20% dos dados)\n","## Armazenar os limites para cada threshold (e.g., PHcutoffs), para ser aplicado posteriormente\n","X['PhysicalHealth'],PHcutoffs =pd.cut(x=X['PhysicalHealth'],bins=5, labels=['bottom20', 'lower20', 'middle20', 'upper20', 'top20'],retbins=True)\n","X['MentalHealth'],MHcutoffs=pd.cut(x=X['MentalHealth'], bins=5, labels=['bottom20', 'lower20', 'middle20', 'upper20', 'top20'],retbins=True)\n","X['SleepTime'],STcutoffs=pd.cut(x=X['SleepTime'], bins=5,labels=['bottom20', 'lower20', 'middle20', 'upper20', 'top20'],retbins=True)\n","\n","\n","## Discretizar BMI usando pontos de corte padrão (9999 garante abranger todos os valores possíveis)\n","X['BMI']=pd.cut(x=X['BMI'], bins=[0, 18.5, 24.9, 29.9,9999], labels = ['slim', 'normal', 'overweight', 'obese'])\n","\n","## Codifica variáveis categóricas, pré-requisito para CategoricalNB\n","encoder = OrdinalEncoder(dtype=np.int64)\n","encoder.fit(X)\n","X = encoder.transform(X)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,stratify=y,random_state=42)\n","\n","## Treinando o modelo de NB Misto\n","nbM1_clf = CategoricalNB(alpha=1e-10)\n","nbM1_clf.fit(X_train,y_train)\n","#print(nbM1_clf.class_log_prior_)\n","\n","## Prevendo a classe de saída para as instâncias de teste\n","## Por padrão, a classe retornada é aquela que maximiza a probabilidade a posteriori\n","y_predM1 = nbM1_clf.predict(X_test)\n","\n","## As probabilidades por classe podem ser observadas com o seguinte comando\n","y_probaM1 = nbM1_clf.predict_proba(X_test)\n","#print(y_probaM1[:10])\n","\n","\n","## Avaliando o desempenho do modelo usando a matriz de confusão, e três métricas \n","## de desempenho: acurácia, recall (sensibilidade) e precisão.\n","\n","cm = confusion_matrix(y_test, y_predM1,labels=nbM1_clf.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nbM1_clf.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","\n","print('Acurácia: {}'.format(round(accuracy_score(y_test, y_predM1),3)))\n","print('Recall: {}'.format(round(recall_score(y_test, y_predM1,pos_label='Yes'),3)))\n","print('Precisão: {}'.format(round(precision_score(y_test, y_predM1,pos_label='Yes'),3)))"],"metadata":{"id":"ajpnwYEPjAFN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","### Aplicando o modelo NB para novos dados\n","\n","**>>>> Exercício:** Assuma que agora um novo arquivo foi liberado com a resposta de novas pessoas que responderam à pesquisa do CDC. Faça o tratamento apropriado dos dados e aplique um dos modelos NB treinados (por exemplo, o modelo NB Misto com a Abordagem 2). Após realizar a predição, avalie os erros detectados em relação aos valores de atributos das instâncias de entrada, averiguando a possibilidade de erros sistemáticos no modelo treinado.\n","\n","Dica: caso deseje usar a Abordagem 2, você pode utilizar os cutoffs calculados anteriormente para discretizar os atributos. por exemplo:\n","`X2['PhysicalHealth']=pd.cut(x=X2['PhysicalHealth'],bins=PHcutoffs, labels=['bottom20', 'lower20', 'middle20', 'upper20', 'top20'])`\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"9X0jvqKunKeY"}},{"cell_type":"code","source":["df_test = pd.read_csv(\"https://drive.google.com/uc?export=view&id=1oWoYNOF8rBzZcWivff6ae4eAAwafNvGQ\")\n","\n","X2 = df_test.drop(['HeartDisease'], axis=1)\n","y2 = df_test['HeartDisease'].values\n","\n","#df_test\n","\n","\n","#### \n","# Adicione aqui o código para pré-processar os dados de acordo com o modelo escolhido\n","# e para realizar a predição das novas instâncias.\n","\n","## Identificamos os índices das colunas para cada tipo de atributo\n","catFeat_idx = np.argwhere(X2.columns.isin(categorical)).ravel()\n","numFeat_idx = np.argwhere(X2.columns.isin(numerical)).ravel()\n","\n","# Codifica variáveis categóricas usando inteiros, pré-requisito para CategoricalNB()\n","encoder = OrdinalEncoder(dtype=np.int64)\n","encoder.fit(X2[categorical])\n","X2[categorical] = encoder.transform(X2[categorical])\n","\n","## A partir dos índices, separamos os atributos por cat ou num\n","X2_test_cat = X2.iloc[: ,catFeat_idx]\n","X2_test_num = X2.iloc[: ,numFeat_idx]\n","\n","\n","# modelo com GaussianNB\n","print('Resultado para variáveis numéricas')\n","y2_predG = nbG_clf.predict(X2_test_num)\n","y2_probaG = nbG_clf.predict_proba(X2_test_num) # probabilidades\n","\n","cm = confusion_matrix(y2, y2_predG,labels=nbG_clf.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nbG_clf.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","\n","print('Acurácia: {}'.format(round(accuracy_score(y2, y2_predG),3)))\n","print('Recall: {}'.format(round(recall_score(y2, y2_predG,pos_label='Yes'),3)))\n","print('Precisão: {}'.format(round(precision_score(y2, y2_predG,pos_label='Yes'),3)))\n","\n","# modelo com CategoricalNB\n","print('Resultado para variáveis categóricas')\n","y2_predC = nbC_clf.predict(X2_test_cat)\n","y2_probaC = nbC_clf.predict_proba(X2_test_cat) # probabilidades\n","\n","cm = confusion_matrix(y2, y2_predC,labels=nbC_clf.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nbC_clf.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","\n","print('Acurácia: {}'.format(round(accuracy_score(y2, y2_predC),3)))\n","print('Recall: {}'.format(round(recall_score(y2, y2_predC,pos_label='Yes'),3)))\n","print('Precisão: {}'.format(round(precision_score(y2, y2_predC,pos_label='Yes'),3)))\n","\n","print('Resultado total')\n","## O que vamos calcular:\n","## ProbGaussian * (ProbCategorical/PriorProb)\n","## Normalizando os resultados para que a probabilidade para cada instância some 1 (norm=l1_)\n","normed_matrix = normalize(y2_probaC*(y2_probaG/nbG_clf.class_prior_), axis=1, norm='l1')\n","#print(np.sum(normed_matrix,axis=1))\n","\n","## Analisamos a coluna (0 ou 1) com valor máximo\n","max_index_col = np.argmax(normed_matrix, axis=1).tolist()\n","\n","## Mapeamos os valores inteiros (0 ou 1) para o nome das classes\n","y2_predM2 = np.array(['No' if y==0 else 'Yes' for y in max_index_col]) \n","\n","cm = confusion_matrix(y2, y2_predM2,labels=nbC_clf.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nbC_clf.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","\n","print('Acurácia: {}'.format(round(accuracy_score(y2, y2_predM2),3)))\n","print('Recall: {}'.format(round(recall_score(y2, y2_predM2,pos_label='Yes'),3)))\n","print('Precisão: {}'.format(round(precision_score(y2, y2_predM2,pos_label='Yes'),3)))\n","\n","\n","# Ao final, você pode observar os acertos/erros da seguinte forma:\n","output = pd.DataFrame({'Pred':y2_predM2, 'Real':y2,'Correct':y2_predM2==y2})\n","print(output)\n","\n","\n","\n","\n","####\n"],"metadata":{"id":"54VTDBohwPz4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","#### Sugestões de experimentos extras:\n","\n","*   Para os experimentos com os dados disponibilizados em arquivo independente, verifique se remover atributos sensíveis como *Race* impacta nos resultados. Também verifique se estratificar atributos categóricos de outra forma (menos categorias, por exemplo) pode aprimorar os resultados\n","*   Teste outras alternativas para agregar a saída dos classificadores Naive Bayes Gaussiano e Categórico. Por exemplo, tente uma votação majoritária entre ambos, ou uma votação majoritária ponderada pelo número de atributos avaliado por cada modelo.\n","*   Explore diferentes valores para o hiperparâmetro $alpha$ do CategoricalNB, e verifique se impacta nos resultados obtidos"],"metadata":{"id":"ve0iOG0Qp4Hg"}}]}