{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CD004 - Aula07_atividades.ipynb","provenance":[{"file_id":"1nbRE28FczrhiBU6yJ-kZBhjU3BSaHlXs","timestamp":1656419723765}],"collapsed_sections":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Especialização em Ciência de Dados - INF/UFRGS e SERPRO**\n","### Disciplina CD004 - Metodologia de Aprendizado de Máquina Supervisionado\n","#### *Profa. Mariana Recamonde-Mendoza (mrmendoza@inf.ufrgs.br)*\n","<br> \n","\n","---\n","***Observação:*** *Este notebook é disponibilizado aos alunos como complemento às aulas síncronas e aos slides preparados pela professora. Desta forma, os principais conceitos são apresentados no material teórico fornecido. O objetivo deste notebook é reforçar os conceitos e demonstrar questões práticas no uso de diferentes algoritmos e estratégias de Aprendizado de Máquina.*\n","\n","\n","---\n"],"metadata":{"id":"0FA12O8Tpjmk"}},{"cell_type":"markdown","source":["<br>\n","\n","## **Aula 07** - **Tópico: Interpretação de Modelos Preditivos**\n","\n","<br>\n","\n","**Objetivo deste notebook**: Exemplificar o uso de diferentes estratégias para interpretar ou explicar modelos preditivos treinados com algoritmos de aprendizado de máquina. Discutiremos relevância de atributos (estimada por árvores ou por permutação), Partial Dependence Plots, e SHAP.\n","<br>\n","\n","---\n","\n"],"metadata":{"id":"VDNPuCNO2tpq"}},{"cell_type":"markdown","source":["\n","\n","##**Predição de 'churn' de clientes de serviço de telecomunicação**\n","\n","Os dados que utilizaremos neste notebook se referem a uma empresa de telecomunicações fictícia que forneceu serviços de telefone residencial e Internet para clientes na Califórnia. O conjunto de dados possui informações sobre os serviços para os quais cada cliente se inscreveu (telefone, várias linhas, internet, segurança online, backup online, proteção de dispositivos, suporte técnico e streaming de TV e filmes), informações da conta do cliente (há quanto tempo eles são clientes, contrato, forma de pagamento, cobrança sem papel, cobranças mensais e cobranças totais) e informações demográficas sobre os clientes (sexo, faixa etária, se eles têm parceiros e dependentes). Há, ainda, uma coluna chamada 'churn', que indica os clientes que desistiram do contrato do serviço no último mês. O objetivo da tarefa preditiva é identificar o churn (i.e., saída) de clientes a partir das informações coletadas. Os dados podem ser acessados neste [link](https://www.kaggle.com/datasets/blastchar/telco-customer-churn).\n"],"metadata":{"id":"DbJNQfaM4ERT"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"jiutqCcGYYM0"}},{"cell_type":"code","source":["## Carregando as bibliotecas básicas necessárias\n","# A primeira linha é incluída para gerar os gráficos logo abaixo dos comandos de plot\n","%matplotlib inline              \n","import pandas as pd             # para análise de dados \n","import matplotlib.pyplot as plt # para visualização de informações\n","import seaborn as sns           # para visualização de informações\n","import numpy as np              # para operações com arrays multidimensionais\n","sns.set()"],"metadata":{"id":"GIOBsC_-Khtg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Carregando os dados\n","\n","Nas células a seguir, vamos repetir o procedimento padrão de carregar os dados e fazer algumas modificações e preparação básica dos dados para a tarefa de aprendizado supervisionado."],"metadata":{"id":"dbr-VwMq6OPG"}},{"cell_type":"code","source":["## Carregando os dados\n","df = pd.read_csv(\"https://drive.google.com/uc?export=view&id=10VrzI8mA2wPvkNIDaLzv-IglPU-Febtf\")#,na_values=\"NA\")\n","df  "],"metadata":{"id":"bPcnFsAB9kv0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Características gerais do dataset\n","print(\"O conjunto de dados possui {} linhas e {} colunas\".format(df.shape[0], df.shape[1]))"],"metadata":{"id":"4BWj00iDltyH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A coluna *'Churn'* contém a classificação de cada instância. Vamos avaliar a distribuição de classes do problema."],"metadata":{"id":"UqoChZTpAEMu"}},{"cell_type":"code","source":["## Distribuição do atributo alvo\n","plt.hist(df['Churn'])\n","plt.title(\"Distribuição do atributo alvo\")\n","plt.show()"],"metadata":{"id":"h0NT9TCiAeLX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"fwVKHnB1SdOM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["O atributo customerID é único para cada instância e no geral não possui valor preditivo. Iremos removê-lo da análise."],"metadata":{"id":"J_EFTXXhkfXC"}},{"cell_type":"code","source":["customer_ids = df['customerID']\n","df = df.drop(['customerID'], axis=1)"],"metadata":{"id":"QC5YOq8Jkeq2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##remove as duplicatas\n","df=df.drop_duplicates(keep='last')"],"metadata":{"id":"lfnumOq4pdvG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como atributos categóricos e numéricos podem demandar pré-processamentos diferentes, vamos separar os respectivos \"nomes\" em dois vetores distintos, facilitando a manipulação dos dados posteriormente."],"metadata":{"id":"UoOevD6R14tT"}},{"cell_type":"code","source":["## Separa os atributos em vetores, de acordo com o tipo de dado (categórico ou numérico)\n","cat_columns=list(df.drop(['Churn'], axis=1).select_dtypes(include=[\"object\"]).columns)\n","print(cat_columns)\n","\n","num_columns=list(df.select_dtypes(include=[\"int64\", \"float64\"]).columns)\n","print(num_columns)"],"metadata":{"id":"XEkRHdAqSS4Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Antes de iniciar o treinamento do modelo, lembre-se que é recomendado sempre reservar uma porção dos dados para teste, a qual somente será utilizada para avaliação do modelo final (após todo o processo de treinamento e otimização de hiperparâmetros).\n","\n","Vamos fazer esta divisão, separando 20% para teste. Entretanto, primeiro precisamos dividir os dados entre atributos (X) e classe (y). Também iremos codificar as classes Yes/No para 1/0."],"metadata":{"id":"mtYK3Fv9_XE3"}},{"cell_type":"code","source":["## Separa o dataset em duas variáveis: os atributos/entradas (X) e a classe/saída (y)\n","X = df.drop(['Churn'], axis=1)\n","y = df['Churn'].values"],"metadata":{"id":"pd1ZSQRnXQq3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Faremos o mapeamento das classes Yes/No para 1/0. Por padrão, as funções de avaliação assumem que a classe 1 é a positiva/de interesse."],"metadata":{"id":"FXQ06lTj_auv"}},{"cell_type":"code","source":["## substitui No' por 0, 'Yes' por 1\n","y = np.array([0 if y=='No' else 1 for y in y]) "],"metadata":{"id":"vXM10ZIZjZzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","## Faz a divisão entre treino (80%) e teste (20%).\n","## O conjunto de treino representa os dados que serão usados\n","## ao longo do desenvolvimento do modelo\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,stratify=y,random_state=42) "],"metadata":{"id":"eZbsoLfsXkoZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","### Pipeline de pré-processamento e treinamento\n","\n","Continuaremos usando o conceito de Pipelines para realizar o pré-processamento de atributos e o treinamento do modelo. Serão aplicados os seguintes passos, conforme notebooks anteriores:\n","\n","*   Atributos **numéricos**: imputação de valores faltantes (com o método KNN) e normalização\n","*   Atributos **categóricos**: imputação de valores faltantes (com a moda), one-hot encoding, e normalização.\n","\n","A última etapa do pipeline é o treinamento do modelo preditivo. Neste notebook, vamos usar o classificador Random Forest. A estratégia utilizada para lidar com desbalanceamento de dados será um aprendizado sensível a custo, através do hiperparâmetro class_weight.\n","\n","**Observação:** Salienta-se que o objetivo deste notebook é entender o funcionamento dos métodos de interpretação de modelos. Esta etapa é normalmente feita ao final de todo o processo de desenvolvimento de modelos preditivos (incluindo a otimização de hiperparâmetros com estratégias como nested cross-validation). Neste notebook vamos simplificar essa análise, usando um único algoritmo e um processo bem rápido de otimização de hiperparâmetros com validação cruzada."],"metadata":{"id":"ZkOKOBleLW0b"}},{"cell_type":"code","source":["## Nesta célula definimos o pipeline e fazemos o ajuste aos dados\n","\n","from sklearn.impute import KNNImputer\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.model_selection import GridSearchCV, StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier\n","\n","cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n","\n","\n","## pipeline específico para os atributos numéricos\n","num_pipeline = Pipeline(steps=[\n","                         ('imputer', KNNImputer(weights=\"uniform\"))])\n","\n","## pipeline específico para os atributos categóricos\n","cat_pipeline = Pipeline(steps=[\n","                         ('imputer', SimpleImputer(strategy='most_frequent')),\n","                         ('encoder', OneHotEncoder(drop='if_binary', sparse=False))])\n","\n","## ColumnTransformer para aplicar cada pipeline ao respectivo tipo de atributo\n","data_pipeline = ColumnTransformer(transformers=[\n","                                   ('numerical', num_pipeline, num_columns),\n","                                   ('categorical', cat_pipeline, cat_columns)])\n","\n","## define pipeline que une as transformações definidas anteriormente e aplica a \n","## normalização em todos os atributos\n","prep_pipeline = Pipeline(steps=[\n","                 ('data_transform', data_pipeline),\n","                 ('data_normalize',MinMaxScaler())])\n","\n","## finalmente, pipeline que executa o pré-processamento dos dados seguido pelo\n","## treinamento do modelo floresta aleatória.\n","tree_pipeline = Pipeline(steps=[\n","                          ('data_preproc', prep_pipeline), \n","                          ('clf',RandomForestClassifier(random_state=42,n_estimators=50,class_weight='balanced_subsample'))])\n","\n","## define uma grid de hiperparâmetros a serem testados e avaliados\n","tree_param_grid = {\n","    'clf__max_depth': [4, 5, 6, 7, 8]\n","}\n","\n","## define e aplica uma grid search. \n","tree_grid_search = GridSearchCV(\n","    estimator=tree_pipeline,\n","    param_grid=tree_param_grid,\n","    scoring='f1',\n","    cv=cv,\n","    refit=True\n",")\n","\n","tree_grid_search = tree_grid_search.fit(X=X_train, y=y_train)\n"],"metadata":{"id":"OTZXGp1O8Z5h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tree_grid_search.best_params_)"],"metadata":{"id":"AsGNkCPURcyE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cria uma versão dos dados pré-processados, de acordo com o pipeline definido acima (pre_proc, sem a etapa de classificação)"],"metadata":{"id":"s25kG3ps-bAo"}},{"cell_type":"code","source":["X_train_prep = tree_pipeline['data_preproc'].fit_transform(X_train)\n","X_test_prep = tree_pipeline['data_preproc'].transform(X_test)\n","columns = np.append(num_columns,prep_pipeline[0].named_transformers_['categorical']['encoder'].get_feature_names_out(cat_columns))\n","df_train_prep = pd.DataFrame(X_train_prep, columns=columns)\n","df_test_prep = pd.DataFrame(X_test_prep, columns=columns)"],"metadata":{"id":"yYsPA6uSsaNz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Iniciamos fazendo uma avaliação do modelo. Para interpretação dos modelos, é sempre interessante que o desempenho quanto a sua capacidade preditiva seja a melhor possível. Nosso modelo possui um recall relativamente bom, mas baixa precisão. Não aprofundaremos a investigação nesse sentido, mas é importante sempre lembrar que a interpretação de modelos é mais produtiva e relevante quando o modelo apresenta potencial de solução da tarefa investigada."],"metadata":{"id":"QB6JGVUt90qq"}},{"cell_type":"code","source":["y_test_pred = tree_grid_search.predict(X=X_test)"],"metadata":{"id":"TeMzjxzzQ4_f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, recall_score, precision_score,accuracy_score,f1_score,ConfusionMatrixDisplay ## para avaliação dos modelos\n","\n","cm = confusion_matrix(y_test, y_test_pred,labels=tree_grid_search.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=tree_grid_search.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","\n","print('Acurácia: {}'.format(round(accuracy_score(y_test, y_test_pred),3)))\n","print('Recall: {}'.format(round(recall_score(y_test, y_test_pred,pos_label=1),3)))\n","print('Precisão: {}'.format(round(precision_score(y_test, y_test_pred,pos_label=1),3)))\n","print('F1-Score: {}'.format(round(f1_score(y_test, y_test_pred,pos_label=1),3)))"],"metadata":{"id":"7Sm9orEcQ-_u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Análise de importância de atributos"],"metadata":{"id":"TJqvA612-mjJ"}},{"cell_type":"markdown","source":["Como vimos na Aula 06, alguns modelos são capazes de estimar uma importância para cada atributo a partir dos dados e do processo de treinamento. Florestas Aleatórias é um algoritmo que possui essa característica, retornando dados úteis para iniciar a interpretação do modelo.\n"],"metadata":{"id":"Y3WIkciYxE3I"}},{"cell_type":"code","source":["feature_importance = tree_grid_search.best_estimator_['clf'].feature_importances_\n","sorted_idx = np.argsort(feature_importance)\n","fig = plt.figure(figsize=(12, 8))\n","plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n","plt.yticks(range(len(sorted_idx)), np.array(columns)[sorted_idx])\n","plt.title('Feature Importance')\n"],"metadata":{"id":"5soas-KLvgll"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Enquanto o gráfico acima demonstra a importância média (entre o ensemble de árvores), também é possível avalair como a importância muda entre as N árvores do ensemble."],"metadata":{"id":"Sznb3Z4NISlh"}},{"cell_type":"code","source":["importances = tree_grid_search.best_estimator_['clf'].feature_importances_\n","std = np.std([tree.feature_importances_ for tree in tree_grid_search.best_estimator_['clf']], axis=0)"],"metadata":{"id":"CJnZinaAtiET"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["forest_importances = pd.Series(tree_grid_search.best_estimator_['clf'].feature_importances_, index=columns)\n","\n","fig, ax = plt.subplots(figsize=(12, 6))\n","forest_importances.plot.bar(yerr=std, ax=ax)\n","ax.set_title(\"Feature importances using MDI\")\n","ax.set_ylabel(\"Mean decrease in impurity\")"],"metadata":{"id":"FbI8t7XTtkkK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A análise de importância de atributos pode ser aplicada a qualquer modelo a partir da estratégia de importância baseada em permutação: a importância de um atributo é estimada como o nível de redução no score de um modelo (onde score é a métrica de desempenho sendo otimizada) ao se permutar aleatoriamente os valores deste atributo.\n","\n","Abaixo vamos aplicar esse método para a Floresta Aleatória. Ele é chamado um método 'agnóstico', pois pode ser usada com qualquer modelo derivado de aprendizado supervisionado. O scikit-learn fornece o método permutation_importance."],"metadata":{"id":"hSoqnw5CIbnS"}},{"cell_type":"code","source":["from sklearn.inspection import permutation_importance\n","\n","## faz permutações, treina e avalia modelos, e estima diferença de desempenho (proxy importância)\n","perm_importance = permutation_importance(tree_grid_search.best_estimator_['clf'], X_train_prep, y_train, n_repeats=30,random_state=0)"],"metadata":{"id":"EmtjmV_xt_s4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Analisando a estimativa de importância para cada atributo."],"metadata":{"id":"-O32QU4IJUn0"}},{"cell_type":"code","source":["sorted_idx = perm_importance.importances_mean.argsort()\n","fig = plt.figure(figsize=(12, 6))\n","plt.barh(range(len(sorted_idx)), perm_importance.importances_mean[sorted_idx], align='center')\n","plt.yticks(range(len(sorted_idx)), np.array(columns)[sorted_idx])\n","plt.title('Permutation Importance')"],"metadata":{"id":"6lmicaa8vXao"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["É interessante percebermos que enquanto a relevância de atributos pode nos ajudar a entender quais atributos mais impactam no modelo, ela não nos informa como ele impacta, isto é, se aumentar ou diminuir o valor do atributo aumenta ou diminui as chances para uma determinada classe. "],"metadata":{"id":"M2706aPRJmQx"}},{"cell_type":"markdown","source":["### Partial dependence plot (PDP)"],"metadata":{"id":"GIbQuhBBJjCE"}},{"cell_type":"markdown","source":["Analisa o efeito marginal que um (ou dois) atributos têm sobre o resultado previsto de um modelo de aprendizado de máquina. Para um determinado valor do atributo analisado, representa a previsão média do modelo para o caso em que forçamos todas as instâncias a assumirem aquele valor. Permite analisar uma relação entre atributo e saída no modelo."],"metadata":{"id":"z2-c2vIDKi76"}},{"cell_type":"markdown","source":["Nas células a seguir, vamos explorar PDP para visualizar a forma como alguns atributos impactam na decisão do modelo."],"metadata":{"id":"ULdMYXxNKq77"}},{"cell_type":"code","source":["from sklearn.inspection import PartialDependenceDisplay\n","\n","features_to_display = ['Contract_Two year','Contract_One year','Contract_Month-to-month','TotalCharges','MonthlyCharges', 'tenure']\n","\n","fig, ax = plt.subplots(figsize=(15, 7))\n","\n","display_tree = PartialDependenceDisplay.from_estimator(\n","       estimator=tree_grid_search.best_estimator_['clf'],\n","       X=df_train_prep,\n","       features = features_to_display,\n","       kind='average',\n","       subsample=50,\n","       n_jobs=3, \n","       grid_resolution=20,\n","       random_state=42,\n","       ax=ax,      \n",")\n","\n","fig.suptitle('PDP ', y=0.95);"],"metadata":{"id":"F-cIoNDDyu8y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features_to_display = ['TechSupport_No internet service','DeviceProtection_No internet service','OnlineSecurity_No internet service',\n","                       'TechSupport_No','DeviceProtection_No','OnlineSecurity_No',\n","                       'TechSupport_Yes','DeviceProtection_Yes','OnlineSecurity_Yes']\n","\n","fig, ax = plt.subplots(figsize=(15, 7))\n","\n","display_tree = PartialDependenceDisplay.from_estimator(\n","       estimator=tree_grid_search.best_estimator_['clf'],\n","       X=df_train_prep,\n","       features = features_to_display,\n","       kind='average',\n","       subsample=50,\n","       n_jobs=3, \n","       grid_resolution=20,\n","       random_state=42,\n","       ax=ax,      \n",")\n","\n","fig.suptitle('PDP ', y=0.95);"],"metadata":{"id":"Qyni6mW-2r48"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features_to_display = [('tenure','TotalCharges'),('MonthlyCharges','Contract_Month-to-month')]\n","\n","fig, ax = plt.subplots(figsize=(10, 6))\n","\n","display_tree = PartialDependenceDisplay.from_estimator(\n","       estimator=tree_grid_search.best_estimator_['clf'],\n","       X=df_train_prep,\n","       features = features_to_display,\n","       kind='average',\n","       subsample=50,\n","       n_jobs=3, \n","       grid_resolution=20,\n","       random_state=42,\n","       ax=ax,      \n",")\n","\n","fig.suptitle('PDP ', y=0.95);"],"metadata":{"id":"2AY5gMvN3-Lc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### SHAP\n","\n","O SHAP explica a previsão de uma instância x calculando a contribuição de cada atributo para a previsão. Valor de Shapley é a contribuição marginal média de um valor de atributo em todas as coalizões possíveis (isto é, combinação entre valores de atributos).\n","\n","Utilizaremos a biblioteca shap.\n","Ler mais em: https://github.com/slundberg/shap"],"metadata":{"id":"SK0OiZIQKx8S"}},{"cell_type":"code","source":["##instalando a biblioteca\n","!pip install shap"],"metadata":{"id":"kvKSHX4c61FV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shap\n","## como o processo de calcular valores SHAP é custoso, usamos apenas uma amostra dos dados\n","X_train_sample = shap.utils.sample(X_train_prep, 500)\n","# compute the SHAP values for the linear model\n","explainer = shap.Explainer(tree_grid_search.best_estimator_['clf'].predict, X_train_sample)\n","shap_values = explainer(X_test_prep)"],"metadata":{"id":"WGSyaDzcOZ7v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#shap.plots.bar(shap_values[0]) ## semelhante ao plots.waterfall.\n","shap.plots.waterfall(shap_values[0], max_display=14) ##analisa a primeira instância de teste (0)"],"metadata":{"id":"8tTOAiG5PClf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(columns[32])"],"metadata":{"id":"-n7F4bcNTEcT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para uma explicação global, o gráfico beeswarm é mais interessante. Os atributos são ordenados por relevância de acordo com análise SHAP e cada ponto representa uma instância de teste. A cor dos pontos e a sua posição em relação ao eixo x permitem verificar se um determinado atributo tende a contribuir ou não para predição de uma classe quando assume determinados valores. Por exemplo, o atributo 32, quando apresenta valores mais altos (nos nossos dados, valor de 1/máximo), contribuir para a predição da classe positiva ('churn')."],"metadata":{"id":"iR-Ntw0QUwnk"}},{"cell_type":"code","source":["shap.plots.beeswarm(shap_values, max_display=20,feature_names=columns)"],"metadata":{"id":"o1PQJ-MhLhfr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shap.plots.bar(shap_values)"],"metadata":{"id":"3fnzezB-GlVO"},"execution_count":null,"outputs":[]}]}