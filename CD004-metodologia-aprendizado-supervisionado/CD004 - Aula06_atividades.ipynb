{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CD004 - Aula06_atividades.ipynb","provenance":[{"file_id":"1a0cOKXdI94ZUECI-IfdbXhmiMaigmBJO","timestamp":1655812603317}],"collapsed_sections":[],"private_outputs":true,"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Especialização em Ciência de Dados - INF/UFRGS e SERPRO**\n","### Disciplina CD004 - Metodologia de Aprendizado de Máquina Supervisionado\n","#### *Profa. Mariana Recamonde-Mendoza (mrmendoza@inf.ufrgs.br)*\n","<br> \n","\n","---\n","***Observação:*** *Este notebook é disponibilizado aos alunos como complemento às aulas síncronas e aos slides preparados pela professora. Desta forma, os principais conceitos são apresentados no material teórico fornecido. O objetivo deste notebook é reforçar os conceitos e demonstrar questões práticas no uso de diferentes algoritmos e estratégias de Aprendizado de Máquina.*\n","\n","\n","---\n"],"metadata":{"id":"0FA12O8Tpjmk"}},{"cell_type":"markdown","source":["<br>\n","\n","## **Aula 06** - **Tópico: Pré-processamento de dados. Seleção de atributos**\n","\n","<br>\n","\n","**Objetivo deste notebook**: Explorar estratégias para mitigar o problema de desbalanceamento de classes, complementando o pipeline da aula anterior que realizava pré-processamento dos dados através da imputação de valores e transformação de dados (codificação, discretização, normalização).\n","<br>\n","\n","---\n","\n"],"metadata":{"id":"VDNPuCNO2tpq"}},{"cell_type":"markdown","source":["\n","\n","##**Predição de 'churn' de clientes de serviço de telecomunicação**\n","\n","Os dados que utilizaremos neste notebook se referem a uma empresa de telecomunicações fictícia que forneceu serviços de telefone residencial e Internet para clientes na Califórnia. O conjunto de dados possui informações sobre os serviços para os quais cada cliente se inscreveu (telefone, várias linhas, internet, segurança online, backup online, proteção de dispositivos, suporte técnico e streaming de TV e filmes), informações da conta do cliente (há quanto tempo eles são clientes, contrato, forma de pagamento, cobrança sem papel, cobranças mensais e cobranças totais) e informações demográficas sobre os clientes (sexo, faixa etária, se eles têm parceiros e dependentes). Há, ainda, uma coluna chamada 'churn', que indica os clientes que desistiram do contrato do serviço no último mês. O objetivo da tarefa preditiva é identificar o churn (i.e., saída) de clientes a partir das informações coletadas. Os dados podem ser acessados neste [link](https://www.kaggle.com/datasets/blastchar/telco-customer-churn).\n"],"metadata":{"id":"DbJNQfaM4ERT"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"jiutqCcGYYM0"}},{"cell_type":"code","source":["## Carregando as bibliotecas básicas necessárias\n","# A primeira linha é incluída para gerar os gráficos logo abaixo dos comandos de plot\n","%matplotlib inline              \n","import pandas as pd             # para análise de dados \n","import matplotlib.pyplot as plt # para visualização de informações\n","import seaborn as sns           # para visualização de informações\n","import numpy as np              # para operações com arrays multidimensionais"],"metadata":{"id":"GIOBsC_-Khtg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Carregando e inspecionando os dados\n","\n","Primeiramente, vamos carregar algumas bibliotecas importantes do Python e os dados a serem utilizados neste estudo. Os dados são disponibilizados através de um link, que também pode ser diretamente acessado pelos alunos."],"metadata":{"id":"dbr-VwMq6OPG"}},{"cell_type":"code","source":["## Bibliotecas para treinamento/avaliação de modelos\n","from sklearn.model_selection import RepeatedKFold, train_test_split, cross_validate, cross_val_score, GridSearchCV\n","from sklearn import metrics\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","sns.set()"],"metadata":{"id":"2lyNg5Sluhbg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Carregando os dados\n","df = pd.read_csv(\"https://drive.google.com/uc?export=view&id=10VrzI8mA2wPvkNIDaLzv-IglPU-Febtf\")#,na_values=\"NA\")\n","df  "],"metadata":{"id":"bPcnFsAB9kv0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Características gerais do dataset\n","print(\"O conjunto de dados possui {} linhas e {} colunas\".format(df.shape[0], df.shape[1]))"],"metadata":{"id":"4BWj00iDltyH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A coluna *'Churn'* contém a classificação de cada instância. Vamos avaliar a distribuição de classes do problema."],"metadata":{"id":"UqoChZTpAEMu"}},{"cell_type":"code","source":["## Distribuição do atributo alvo\n","plt.hist(df['Churn'])\n","plt.title(\"Distribuição do atributo alvo\")\n","plt.show()"],"metadata":{"id":"h0NT9TCiAeLX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"fwVKHnB1SdOM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["O atributo customerID é único para cada instância e no geral não possui valor preditivo. Iremos removê-lo da análise."],"metadata":{"id":"J_EFTXXhkfXC"}},{"cell_type":"code","source":["customer_ids = df['customerID']\n","df = df.drop(['customerID'], axis=1)"],"metadata":{"id":"QC5YOq8Jkeq2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##remove as duplicatas\n","df=df.drop_duplicates(keep='last')"],"metadata":{"id":"lfnumOq4pdvG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como atributos categóricos e numéricos podem demandar pré-processamentos diferentes, vamos separar os respectivos \"nomes\" em dois vetores distintos, facilitando a manipulação dos dados posteriormente."],"metadata":{"id":"UoOevD6R14tT"}},{"cell_type":"code","source":["## Separa os atributos em vetores, de acordo com o tipo de dado (categórico ou numérico)\n","cat_columns=list(df.drop(['Churn'], axis=1).select_dtypes(include=[\"object\"]).columns)\n","print(cat_columns)\n","\n","num_columns=list(df.select_dtypes(include=[\"int64\", \"float64\"]).columns)\n","print(num_columns)"],"metadata":{"id":"XEkRHdAqSS4Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","\n","\n","### Criando conjuntos de treino e teste para avaliação de modelos\n","\n","\n","Antes de iniciar o treinamento do modelo, lembre-se que é recomendado sempre reservar uma porção dos dados para teste, a qual somente será utilizada para avaliação do modelo final (após todo o processo de treinamento e otimização de hiperparâmetros).\n","\n","Vamos fazer esta divisão, separando 20% para teste. Entretanto, primeiro precisamos dividir os dados entre atributos (X) e classe (y). Também iremos codificar as classes Yes/No para 1/0.\n","\n"],"metadata":{"id":"63nQ-E0jVQzW"}},{"cell_type":"code","source":["## Separa o dataset em duas variáveis: os atributos/entradas (X) e a classe/saída (y)\n","X = df.drop(['Churn'], axis=1)\n","y = df['Churn'].values"],"metadata":{"id":"pd1ZSQRnXQq3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Faremos o mapeamento das classes Yes/No para 1/0. Por padrão, as funções de avaliação assumem que a classe 1 é a positiva/de interesse."],"metadata":{"id":"AfWwwt-QiB16"}},{"cell_type":"code","source":["## substitui No' por 0, 'Yes' por 1\n","y = np.array([0 if y=='No' else 1 for y in y]) "],"metadata":{"id":"vXM10ZIZjZzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Faz a divisão entre treino (80%) e teste (20%).\n","## O conjunto de treino representa os dados que serão usados\n","## ao longo do desenvolvimento do modelo\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,stratify=y,random_state=42) "],"metadata":{"id":"eZbsoLfsXkoZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","## Pré-Processamento dos Dados\n","\n","Continuaremos usando o conceito de Pipelines para realizar o pré-processamento de atributos. Serão aplicados os seguintes passos, conforme notebooks anteriores:\n","\n","*   Atributos **numéricos**: imputação de valores faltantes (com o método KNN) e normalização\n","*   Atributos **categóricos**: imputação de valores faltantes (com a moda), one-hot encoding, e normalização.\n","\n","**Observação:** Salienta-se que como o objetivo deste notebook é observar os resultados dos métodos de redução de dimensionalidade, a aplicação destes métodos e o subsequente treinamento de modelos não será realizado com nested/k-fold cross-validation, mas sim com um simples holdout. Isto facilita a manipulação dos dados e a visualização dos seus resultados. Na prática, os métodos de redução de dimensionalidade devem ser inseridos no Pipeline que será executado a cada iteração do processo de treinamento e validação dos modelos.\n"],"metadata":{"id":"ZkOKOBleLW0b"}},{"cell_type":"code","source":["from sklearn.impute import KNNImputer\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","\n","## pipeline específico para os atributos numéricos\n","num_pipeline = Pipeline([\n","                         ('imputer', KNNImputer(weights=\"uniform\"))])\n","\n","## pipeline específico para os atributos categóricos\n","cat_pipeline = Pipeline([\n","                         ('imputer', SimpleImputer(strategy='most_frequent')),\n","                         ('encoder', OneHotEncoder(drop='if_binary', sparse=False))])\n","\n","## ColumnTransformer para aplicar cada pipeline ao respectivo tipo de atributo\n","data_pipeline = ColumnTransformer([\n","                                   ('numerical', num_pipeline, num_columns),\n","                                   ('categorical', cat_pipeline, cat_columns)])\n","\n","## define pipeline que une as transformações definidas anteriormente e aplica a \n","## normalização com método StandardScaler() em todos os atributos\n","prep_pipeline = Pipeline([\n","                 ('data_transform', data_pipeline),\n","                 ('data_normalize',StandardScaler())])\n","\n","## ajusta o pipeline a partir dos dados de treino, e na sequência aplica em \n","## treino e teste separadamente\n","prep_pipeline.fit(X_train)\n","X_train_prep = prep_pipeline.transform(X_train)\n","X_test_prep = prep_pipeline.transform(X_test)"],"metadata":{"id":"cKincUm4BBbT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ajusta nome das columas e mostra dataframe após pré-processamento\n","columns = np.append(num_columns,prep_pipeline[0].named_transformers_['categorical']['encoder'].get_feature_names_out(cat_columns))\n","df_train_prep = pd.DataFrame(X_train_prep, columns=columns)\n","df_test_prep = pd.DataFrame(X_test_prep, columns=columns)"],"metadata":{"id":"yfd_tvRj_iY3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualizando o formato do conjunto de dados de treinamento após aplicação do Pipeline de pré-processamento de dados."],"metadata":{"id":"GVoCW6jfGO3J"}},{"cell_type":"code","source":["df_train_prep"],"metadata":{"id":"_TOF0mezBNXD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Características gerais do dataset após pré-processamento\n","print(\"O conjunto de dados possui {} linhas e {} colunas referentes a atributos\".format(df_train_prep.shape[0], df_train_prep.shape[1]))"],"metadata":{"id":"ya6E2cUAGgge"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","## Redução de dimensionalidade com seleção de atributos por Filtro"],"metadata":{"id":"am_irQPgrWBp"}},{"cell_type":"markdown","source":["O módulo [sklearn.feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html) possui uma série de funções para selecionar atributos, removendo aqueles que parecem não ser relevantes para uma determinada tarefa preditiva. As seções a seguir avaliam as estratégias baseadas em filtro usando o método SelectKBest(). Este método aplica um critério para ordenação dos atributos de acordo com sua relevância ou poder discriminativo e então retém apenas um subconjunto dos mais relevantes, de acordo com valor 'k' informado na chamada ao método.\n","\n","Para classificação, o sklearn disponibiliza dois critérios para estimar importância de atributos: f_classif (baseado no ANOVA) e mutual_inf_classif (baseado na análise de informação mútua)"],"metadata":{"id":"sG0otCr5rzYv"}},{"cell_type":"code","source":["from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_classif\n","from sklearn.metrics import f1_score\n","from sklearn.svm import SVC\n","\n","f_selected = list()\n","fs_perf = []\n","for ii in range(1, df_train_prep.shape[1]):\n","  fs = SelectKBest(score_func=f_classif, k=ii)\n","  X_train_fsc = fs.fit_transform(X_train_prep, y_train)\n","  X_test_fsc = fs.transform(X_test_prep)\n","\n","  cols = fs.get_support(indices=True)\n","  f_selected.append((str(ii),columns[cols]))\n","\n","  clf_fs = SVC(kernel='rbf', C=0.1)\n","  clf_fs = clf_fs.fit(X_train_fsc,y_train)\n","  clf_fs_pred = clf_fs.predict(X_test_fsc)\n","\n","  fs_perf.append(round(f1_score(y_test, clf_fs_pred),3))"],"metadata":{"id":"blfcsf4bu10p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12, 6))\n","plt.xlabel(\"Number of features selected\")\n","plt.ylabel(\"F1-score\")\n","plt.plot(range(1,40), fs_perf, color='steelblue', linestyle='dashed', marker='o', markerfacecolor='darkblue', markersize=10)\n","plt.show()"],"metadata":{"id":"ka0J3DvBEhei"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f_selected[0])"],"metadata":{"id":"naFqarm1AY1J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f_selected[0][1])\n","print(f_selected[1][1])\n","print(f_selected[2][1])\n","print(f_selected[3][1])"],"metadata":{"id":"FfLcEbtuPOoD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Com Mutual Information"],"metadata":{"id":"2lGxUK2sB2HS"}},{"cell_type":"code","source":["from sklearn.feature_selection import mutual_info_classif\n","\n","f_selected = list()\n","fs_perf = []\n","for ii in range(1, df_train_prep.shape[1]):\n","  fs = SelectKBest(score_func=mutual_info_classif, k=ii)\n","  X_train_fsc = fs.fit_transform(X_train_prep, y_train)\n","  X_test_fsc = fs.transform(X_test_prep)\n","\n","  cols = fs.get_support(indices=True)\n","  f_selected.append((str(ii),columns[cols]))\n","\n","  clf_fs = SVC(kernel='rbf', C=0.1)\n","  clf_fs = clf_fs.fit(X_train_fsc,y_train)\n","  clf_fs_pred = clf_fs.predict(X_test_fsc)\n","\n","  fs_perf.append(round(f1_score(y_test, clf_fs_pred),3))"],"metadata":{"id":"1cQQ3p0-B1hM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12, 6))\n","plt.xlabel(\"Number of features selected\")\n","plt.ylabel(\"F1-score\")\n","plt.plot(range(1,40), fs_perf, color='steelblue', linestyle='dashed', marker='o', markerfacecolor='darkblue', markersize=10)\n","plt.show()"],"metadata":{"id":"qYOFShXbB8ER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f_selected[0][1])\n","print(f_selected[1][1])\n","print(f_selected[2][1])\n","print(f_selected[3][1])"],"metadata":{"id":"xcvWKa3gCHSr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","## Redução de dimensionalidade com seleção de atributos por métodos embedded\n","\n","Alguns algoritmos de aprendizado de máquina são capazes de atribuir uma importância a cada atributo durante o processo de treinamento. No sklearn, estas informações estão normalmente nos atributos *coef_* ou *feature_importances_*."],"metadata":{"id":"dHiC9wRzD1eO"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_iris\n","from sklearn.feature_selection import SelectFromModel\n","\n","clf = RandomForestClassifier(n_estimators=50,random_state=42)\n","clf = clf.fit(X_train_prep, y_train)\n","importances = clf.feature_importances_\n","std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)"],"metadata":{"id":"0C5PzbEGEYNH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para florestas aleatórias, por ser um ensemble de árvores, podemos analisar a variação das importâncias atribuídas entre as árvores."],"metadata":{"id":"8YESzezXN8R0"}},{"cell_type":"code","source":["forest_importances = pd.Series(clf.feature_importances_, index=columns)\n","\n","fig, ax = plt.subplots(figsize=(12, 6))\n","forest_importances.plot.bar(yerr=std, ax=ax)\n","ax.set_title(\"Feature importances using MDI\")\n","ax.set_ylabel(\"Mean decrease in impurity\")"],"metadata":{"id":"fEoNrkktWYiu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Aplicando o método SelectFromModel sobre o Floresta Aleatória. Por padrão SelectFromModel selecionará os atributos cuja importância é maior que a importância média de todos os atributos."],"metadata":{"id":"HMZwWU0IYEuK"}},{"cell_type":"code","source":["model = SelectFromModel(clf, prefit=True)\n","X_preproc_fs = model.transform(X_train_prep)\n","\n","print(X_preproc_fs.shape[1])"],"metadata":{"id":"WT86gsURXlEe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features_idx = model.get_support()\n","features_name = columns[features_idx]\n","print(features_name)"],"metadata":{"id":"Z90EYL5oYU1F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","## Redução de dimensionalidade com seleção de atributos por métodos wrapper"],"metadata":{"id":"GDae2sqJPpPO"}},{"cell_type":"markdown","source":["O sklearn disponibiliza diferentes métodos de seleção de atributos por wrapper, incluindo o RFE. Além disso, a biblioteca fornece o método RFECV, que automaticamente determina o melhor tamanho de subconjunto de atributos para selecionar. A célula abaixo exemplifica o uso do RFECV. Os resultados estão condicionados ao uso do SVC linear como método de avaliação de importância de atributos."],"metadata":{"id":"GANryTvoPvP6"}},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedKFold\n","from sklearn.feature_selection import RFECV\n","\n","svc = SVC(kernel='linear', C=0.1) ## apenas o SVM linear possui 'coef_' para estimar importância de atributos\n","\n","min_features_to_select = 1  # Número mínimo de atributos para considerar\n","\n","## declara a estrutura do RFE-CV\n","rfecv = RFECV(\n","    estimator=svc,\n","    step=1,\n","    cv=StratifiedKFold(n_splits = 2),\n","    scoring=\"f1\",\n","    min_features_to_select=min_features_to_select,\n",")\n","rfecv.fit(X_train_prep, y_train)"],"metadata":{"id":"dDqANhMWPo0w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Número 'ótimo' de atributos : %d\" % rfecv.n_features_)\n","\n","# Plot number of features VS. cross-validation scores\n","plt.figure(figsize=(12, 6))\n","plt.xlabel(\"Number of features selected\")\n","plt.ylabel(\"F1-score\")\n","plt.plot(range(min_features_to_select, len(rfecv.grid_scores_) + min_features_to_select),rfecv.grid_scores_,)\n","plt.show()"],"metadata":{"id":"pfKbA1vNRm0P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sumarizando os atributos e o resultado da seleção de atributos (se foi selecionado ou não, e o respectivo ranking)"],"metadata":{"id":"RHYMJ4LpOJ7M"}},{"cell_type":"code","source":["for i in range(X_train_prep.shape[1]):\n","\tprint('Column: %d (%s), Selected %s, Rank: %.3f' % (i, columns[i], rfecv.support_[i], rfecv.ranking_[i]))"],"metadata":{"id":"YOWUfE0gR2CH"},"execution_count":null,"outputs":[]}]}