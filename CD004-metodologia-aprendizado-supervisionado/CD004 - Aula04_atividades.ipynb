{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CD004 - Aula04_atividades.ipynb","provenance":[{"file_id":"15gmz8fSowaOZmB4ysN-nw8152oFpbfvG","timestamp":1654865874078}],"collapsed_sections":[],"private_outputs":true,"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Especialização em Ciência de Dados - INF/UFRGS e SERPRO**\n","### Disciplina CD004 - Metodologia de Aprendizado de Máquina Supervisionado\n","#### *Profa. Mariana Recamonde-Mendoza (mrmendoza@inf.ufrgs.br)*\n","<br> \n","\n","---\n","***Observação:*** *Este notebook é disponibilizado aos alunos como complemento às aulas síncronas e aos slides preparados pela professora. Desta forma, os principais conceitos são apresentados no material teórico fornecido. O objetivo deste notebook é reforçar os conceitos e demonstrar questões práticas no uso de diferentes algoritmos e estratégias de Aprendizado de Máquina.*\n","\n","\n","---\n"],"metadata":{"id":"0FA12O8Tpjmk"}},{"cell_type":"markdown","source":["<br>\n","\n","## **Aula 04** - **Tópico: Pré-processamento de dados. Desbalanceamento de classes**\n","\n","<br>\n","\n","**Objetivo deste notebook**: Explorar estratégias para mitigar o problema de desbalanceamento de classes, complementando o pipeline da aula anterior que realizava pré-processamento dos dados através da imputação de valores e transformação de dados (codificação, discretização, normalização).\n","<br>\n","\n","---\n","\n"],"metadata":{"id":"VDNPuCNO2tpq"}},{"cell_type":"markdown","source":["\n","\n","##**Predição de 'churn' de clientes de serviço de telecomunicação**\n","\n","Os dados que utilizaremos neste notebook se referem a uma empresa de telecomunicações fictícia que forneceu serviços de telefone residencial e Internet para clientes na Califórnia. O conjunto de dados possui informações sobre os serviços para os quais cada cliente se inscreveu (telefone, várias linhas, internet, segurança online, backup online, proteção de dispositivos, suporte técnico e streaming de TV e filmes), informações da conta do cliente (há quanto tempo eles são clientes, contrato, forma de pagamento, cobrança sem papel, cobranças mensais e cobranças totais) e informações demográficas sobre os clientes (sexo, faixa etária, se eles têm parceiros e dependentes). Há, ainda, uma coluna chamada 'churn', que indica os clientes que desistiram do contrato do serviço no último mês. O objetivo da tarefa preditiva é identificar o churn (i.e., saída) de clientes a partir das informações coletadas. Os dados podem ser acessados neste [link](https://www.kaggle.com/datasets/blastchar/telco-customer-churn).\n","\n","Este é um problema de predição que usualmente apresenta **classes desbalanceadas**. Neste notebook vamos adicionar ao pipeline de análise dos dados estratégias para tratar o desbalanceamento de classes."],"metadata":{"id":"DbJNQfaM4ERT"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"jiutqCcGYYM0"}},{"cell_type":"code","source":["## Carregando as bibliotecas básicas necessárias\n","# A primeira linha é incluída para gerar os gráficos logo abaixo dos comandos de plot\n","%matplotlib inline              \n","import pandas as pd             # para análise de dados \n","import matplotlib.pyplot as plt # para visualização de informações\n","import seaborn as sns           # para visualização de informações\n","import numpy as np              # para operações com arrays multidimensionais"],"metadata":{"id":"GIOBsC_-Khtg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Carregando e inspecionando os dados\n","\n","Primeiramente, vamos carregar algumas bibliotecas importantes do Python e os dados a serem utilizados neste estudo. Os dados são disponibilizados através de um link, que também pode ser diretamente acessado pelos alunos."],"metadata":{"id":"dbr-VwMq6OPG"}},{"cell_type":"code","source":["## Bibliotecas para treinamento/avaliação de modelos\n","from sklearn.model_selection import RepeatedKFold, train_test_split, cross_validate, cross_val_score, GridSearchCV\n","from sklearn import metrics\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","sns.set()"],"metadata":{"id":"2lyNg5Sluhbg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!pip install missingno ## instalando biblioteca para visualizar valores faltantes"],"metadata":{"id":"rf_FwP-d5pty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import missingno as msno"],"metadata":{"id":"Vg8iB83m5w3G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Carregando os dados\n","df = pd.read_csv(\"https://drive.google.com/uc?export=view&id=10VrzI8mA2wPvkNIDaLzv-IglPU-Febtf\")#,na_values=\"NA\")\n","df  "],"metadata":{"id":"bPcnFsAB9kv0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Características gerais do dataset\n","print(\"O conjunto de dados possui {} linhas e {} colunas\".format(df.shape[0], df.shape[1]))"],"metadata":{"id":"4BWj00iDltyH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A coluna *'Churn'* contém a classificação de cada instância. Vamos avaliar a distribuição de classes do problema."],"metadata":{"id":"UqoChZTpAEMu"}},{"cell_type":"code","source":["## Distribuição do atributo alvo\n","plt.hist(df['Churn'])\n","plt.title(\"Distribuição do atributo alvo\")\n","plt.show()"],"metadata":{"id":"h0NT9TCiAeLX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Quais os tipos de atributos nos dados? Os dados possuem valores faltantes?\n","Podemos responder estas perguntas utilizando o método .info() para um dataframe."],"metadata":{"id":"ap9Axa7H1fpD"}},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"fwVKHnB1SdOM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A biblioteca missingno permite visualizar os dados faltantes como uma matriz. Outras opções de visualização estão disponíveis e podem ser consultadas na documentação da biblioteca."],"metadata":{"id":"rqyBrvdS1tFC"}},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"id":"9aSnCISJ53EE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["O atributo customerID é único para cada instância e no geral não possui valor preditivo. Iremos removê-lo da análise."],"metadata":{"id":"J_EFTXXhkfXC"}},{"cell_type":"code","source":["customer_ids = df['customerID']\n","df = df.drop(['customerID'], axis=1)"],"metadata":{"id":"QC5YOq8Jkeq2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##remove as duplicatas\n","df=df.drop_duplicates(keep='last')"],"metadata":{"id":"lfnumOq4pdvG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como atributos categóricos e numéricos podem demandar pré-processamentos diferentes, vamos separar os respectivos \"nomes\" em dois vetores distintos, facilitando a manipulação dos dados posteriormente."],"metadata":{"id":"UoOevD6R14tT"}},{"cell_type":"code","source":["## Separa os atributos em vetores, de acordo com o tipo de dado (categórico ou numérico)\n","cat_columns=list(df.drop(['Churn'], axis=1).select_dtypes(include=[\"object\"]).columns)\n","print(cat_columns)\n","\n","num_columns=list(df.select_dtypes(include=[\"int64\", \"float64\"]).columns)\n","print(num_columns)"],"metadata":{"id":"XEkRHdAqSS4Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inspecionando a distribuição dos dados numéricos e categóricos:"],"metadata":{"id":"RhmuEpuh2J8f"}},{"cell_type":"code","source":["def dist_plot(df,columns,type='boxplot'):\n","    plt.figure(figsize=(16, 5))\n","    for indx, var  in enumerate(columns):\n","        plt.subplot(1, 3, indx+1)\n","        if (type=='boxplot'):\n","          g = sns.boxplot(x=var, data=df,showfliers=True)\n","        else: \n","          if (type=='histogram'):\n","            g = sns.histplot(x=var, data=df)\n","    plt.tight_layout()\n","\n","def count_plot(df,columns):\n","    plt.figure(figsize=(20, 12))\n","    for indx, var  in enumerate(columns):\n","        plt.subplot(6, 3, indx+1)\n","        g = sns.countplot(x=var, data=df)\n","    plt.tight_layout()"],"metadata":{"id":"45aavabCVANv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count_plot(df,cat_columns)"],"metadata":{"id":"YJ0cj1hbmIUo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dist_plot(df,num_columns)#,type=\"histogram\")"],"metadata":{"id":"DcfYrcxOlASp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","\n","\n","### Criando conjuntos de treino e teste para avaliação de modelos\n","\n","\n","Antes de iniciar o treinamento do modelo, lembre-se que é recomendado sempre reservar uma porção dos dados para teste, a qual somente será utilizada para avaliação do modelo final (após todo o processo de treinamento e otimização de hiperparâmetros).\n","\n","Vamos fazer esta divisão, separando 20% para teste. Entretanto, primeiro precisamos dividir os dados entre atributos (X) e classe (y). Também iremos codificar as classes Yes/No para 1/0.\n","\n"],"metadata":{"id":"63nQ-E0jVQzW"}},{"cell_type":"code","source":["## Separa o dataset em duas variáveis: os atributos/entradas (X) e a classe/saída (y)\n","X = df.drop(['Churn'], axis=1)\n","y = df['Churn'].values"],"metadata":{"id":"pd1ZSQRnXQq3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Faremos o mapeamento das classes Yes/No para 1/0. Por padrão, as funções de avaliação assumem que a classe 1 é a positiva/de interesse."],"metadata":{"id":"AfWwwt-QiB16"}},{"cell_type":"code","source":["## substitui No' por 0, 'Yes' por 1\n","y = np.array([0 if y=='No' else 1 for y in y]) "],"metadata":{"id":"vXM10ZIZjZzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Faz a divisão entre treino (80%) e teste (20%).\n","## O conjunto de treino representa os dados que serão usados\n","## ao longo do desenvolvimento do modelo\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,stratify=y,random_state=42) "],"metadata":{"id":"eZbsoLfsXkoZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","## Revisando o pré-processamento e treinamento com Pipelines\n","\n","Nesta seção, vamos aplicar os pré-processamentos discutidos na aula anterior:\n","\n","*   Atributos **numéricos**: imputação de valores faltantes (com o método KNN) e normalização\n","*   Atributos **categóricos**: imputação de valores faltantes (com a moda), one-hot encoding, e normalização.\n","\n","O último passo é, então, treinar um modelo. \n","\n","Podemos usar um Nested Cross-Validation para realizar otimização de hiperparâmetros e análise de desempenho do modelo, conforme visto no notebook da aula anterior. Entretanto, o processo de Nested CV é mais custoso computacionalmente, demandando mais tempo. Assim, as células a seguir realização apenas um k-fold cross-validation simples com o intuito de comparar os desempenhos com e sem a aplicação de estratégias para ajustar o desbalanceamento de dados, mantendo-se fixo o algoritmo de aprendizado supervisionado a ser usado. Entretanto, em projetos e aplicações reais, recomenda-se o uso de nested cross-validation para otimização dos modelos, conforme já foi discutido em aula,\n"],"metadata":{"id":"ZkOKOBleLW0b"}},{"cell_type":"code","source":["from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n","from sklearn import metrics\n","from sklearn.impute import KNNImputer\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.svm import SVC\n","\n","\n","## pipeline específico para os atributos numéricos\n","num_pipeline = Pipeline([\n","    ('imputer', KNNImputer(weights=\"uniform\"))])\n","\n","## pipeline específico para os atributos categóricos\n","cat_pipeline = Pipeline([\n","                         ('imputer', SimpleImputer(strategy='most_frequent')),\n","                         ('encoder', OneHotEncoder(drop='if_binary', sparse=False))])\n","\n","## ColumnTransformer para aplicar cada pipeline ao respectivo tipo de atributo\n","data_pipeline = ColumnTransformer([\n","                                   ('numerical', num_pipeline, num_columns),\n","                                   ('categorical', cat_pipeline, cat_columns)])\n","\n","## pipeline que une as transformações definidas anteriormente e aplica a \n","## normalização em todos os atributos\n","my_pipeline = Pipeline([\n","                 ('data_transform', data_pipeline),\n","                 ('data_normalize',MinMaxScaler()),\n","                 ('classifier',SVC(kernel='rbf', C=0.1))])\n","\n","# avalia o pipeline\n","cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1) ## usaremos o mesmo nas demais comparações\n","scores = cross_val_score(my_pipeline, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)"],"metadata":{"id":"MXDD3rWHItYS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Média F1: %.3f' % np.mean(scores))\n","print('Desvio Padrão F1: %.3f' % np.std(scores))"],"metadata":{"id":"PEzYQvBZ5jLy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","## Ajustando o desbalanceamento de classes com SMOTE\n","\n","Nesta seção vamos repetir o experimento anterior, mas utilizando o SMOTE para ajustar o desbalanceamento de classes. O Pipeline padrão do sklearn não suporta o uso de método da blblioteca imblearn. Portanto, importamos também a implementação de Pipeline desta biblioteca (no código referido como imbpipeline)"],"metadata":{"id":"myRlYH1u3oms"}},{"cell_type":"code","source":["from imblearn.over_sampling import SMOTE\n","from imblearn.pipeline import Pipeline as imbpipeline\n","\n","\n","## pipeline específico para os atributos numéricos\n","num_pipeline = Pipeline([\n","    ('imputer', KNNImputer(weights=\"uniform\"))])\n","\n","## pipeline específico para os atributos categóricos\n","cat_pipeline = Pipeline([\n","                         ('imputer', SimpleImputer(strategy='most_frequent')),\n","                         ('encoder', OneHotEncoder(drop='if_binary', sparse=False))])\n","\n","## ColumnTransformer para aplicar cada pipeline ao respectivo tipo de atributo\n","data_pipeline = ColumnTransformer([\n","                                   ('numerical', num_pipeline, num_columns),\n","                                   ('categorical', cat_pipeline, cat_columns)])\n","\n","\n","\n","## pipeline que une as transformações definidas anteriormente e aplica a \n","## normalização em todos os atributos, seguida por SMOTE (oversampling)\n","my_pipeline2 = imbpipeline(steps = [['data_transform', data_pipeline],\n","                                    ['scaler', MinMaxScaler()],\n","                                    ['smote', SMOTE(random_state=11)],\n","                                    ['classifier', SVC(kernel='rbf', C=0.1)]])\n","\n","\n","# avalia o pipeline\n","scores2 = cross_val_score(my_pipeline2, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)\n"],"metadata":{"id":"jhn40lSkFI40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Média F1: %.3f' % np.mean(scores2))\n","print('Desvio Padrão F1: %.3f' % np.std(scores2))"],"metadata":{"id":"g2sZ6bbT522e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","## Ajustando o desbalanceamento de classes com Undersampling\n","\n","Nesta seção utilizaremos a abordagem de realizar undersampling a partir da classe majoritária. Na biblioteca imblearn, o método RandomUnderSampler implementa esta estratégia."],"metadata":{"id":"jy_ov3ECShhs"}},{"cell_type":"code","source":["from imblearn.under_sampling import RandomUnderSampler \n","\n","## pipeline específico para os atributos numéricos\n","num_pipeline = Pipeline([\n","    ('imputer', KNNImputer(weights=\"uniform\"))])\n","\n","## pipeline específico para os atributos categóricos\n","cat_pipeline = Pipeline([\n","                         ('imputer', SimpleImputer(strategy='most_frequent')),\n","                         ('encoder', OneHotEncoder(drop='if_binary', sparse=False))])\n","\n","## ColumnTransformer para aplicar cada pipeline ao respectivo tipo de atributo\n","data_pipeline = ColumnTransformer([\n","                                   ('numerical', num_pipeline, num_columns),\n","                                   ('categorical', cat_pipeline, cat_columns)])\n","\n","\n","## pipeline que une as transformações definidas anteriormente e aplica a \n","## normalização em todos os atributos, seguida por undersampling\n","my_pipeline3 = imbpipeline(steps = [['data_transform', data_pipeline],\n","                                    ['scaler', MinMaxScaler()],\n","                                    ['under', RandomUnderSampler(random_state=11)],\n","                                    ['classifier', SVC(kernel='rbf', C=0.1)]])\n","\n","\n","# avalia o pipeline\n","scores3 = cross_val_score(my_pipeline3, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)\n"],"metadata":{"id":"a7EeTreuS0bV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Média F1: %.3f' % np.mean(scores3))\n","print('Desvio Padrão F1: %.3f' % np.std(scores3))"],"metadata":{"id":"-4xTPG6fUALt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","## Ajustando o desbalanceamento de classes com SMOTE+Tomek links\n","\n","A abordagem de realizar oversampling com SMOTE e remover instâncias da classe majoritária com o Tomek links também é implementada na biblioteca imblearn."],"metadata":{"id":"aVXguQAkUPAU"}},{"cell_type":"code","source":["from imblearn.combine import SMOTETomek \n","\n","## pipeline específico para os atributos numéricos\n","num_pipeline = Pipeline([\n","    ('imputer', KNNImputer(weights=\"uniform\"))])\n","\n","## pipeline específico para os atributos categóricos\n","cat_pipeline = Pipeline([\n","                         ('imputer', SimpleImputer(strategy='most_frequent')),\n","                         ('encoder', OneHotEncoder(drop='if_binary', sparse=False))])\n","\n","## ColumnTransformer para aplicar cada pipeline ao respectivo tipo de atributo\n","data_pipeline = ColumnTransformer([\n","                                   ('numerical', num_pipeline, num_columns),\n","                                   ('categorical', cat_pipeline, cat_columns)])\n","\n","\n","\n","## pipeline que une as transformações definidas anteriormente e aplica a \n","## normalização em todos os atributos, seguida por SMOTE+TOMEK (over + undersampling)\n","my_pipeline4 = imbpipeline(steps = [['data_transform', data_pipeline],\n","                                    ['scaler', MinMaxScaler()],\n","                                    ['smotetomek', SMOTETomek(random_state=11)],\n","                                    ['classifier', SVC(kernel='rbf', C=0.1)]])\n","\n","\n","# avalia o pipeline\n","scores4 = cross_val_score(my_pipeline4, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)\n"],"metadata":{"id":"z3yDln5DUwFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Média F1: %.3f' % np.mean(scores4))\n","print('Desvio Padrão F1: %.3f' % np.std(scores4))"],"metadata":{"id":"8zn8eO9_VQOM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Comparando os resultados com amostragem de dados"],"metadata":{"id":"ENhumdWtAxlC"}},{"cell_type":"code","source":["results=[]\n","results.append(scores)\n","results.append(scores2)\n","results.append(scores3)\n","results.append(scores4)\n","plt.boxplot(results, labels=['Original','Smote','UnderSamp','SmoteTomek'], showmeans=True)\n","plt.show()"],"metadata":{"id":"NiKHklSO5swm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","## Ajustando o desbalanceamento de classes com cost-sensitive learning\n","\n","Nesta seção, vamos explorar a abordagem de cost-sensitive learning, ajustando o parâmetro class_weight do método SVC()."],"metadata":{"id":"tCfiEmjGA38_"}},{"cell_type":"code","source":["from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n","from sklearn import metrics\n","from sklearn.impute import KNNImputer\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.svm import SVC\n","\n","\n","## pipeline específico para os atributos numéricos\n","num_pipeline = Pipeline([\n","    ('imputer', KNNImputer(weights=\"uniform\"))])\n","\n","## pipeline específico para os atributos categóricos\n","cat_pipeline = Pipeline([\n","                         ('imputer', SimpleImputer(strategy='most_frequent')),\n","                         ('encoder', OneHotEncoder(drop='if_binary', sparse=False))])\n","\n","## ColumnTransformer para aplicar cada pipeline ao respectivo tipo de atributo\n","data_pipeline = ColumnTransformer([\n","                                   ('numerical', num_pipeline, num_columns),\n","                                   ('categorical', cat_pipeline, cat_columns)])\n","\n","## pipeline que une as transformações definidas anteriormente e aplica a \n","## normalização em todos os atributos\n","my_pipeline5 = Pipeline([\n","                 ('data_transform', data_pipeline),\n","                 ('data_normalize',MinMaxScaler()),\n","                 ('classifier',SVC(kernel='rbf', C=0.1))])\n","\n","\n","# Set of parameters for which to assess model performances\n","param_grid = {'classifier__class_weight':[{0: w} for w in [0.01, 0.05, 0.1, 0.5, 1]]}\n","\n","# avalia o pipeline\n","\n","# define a estratégia de busca dos melhores hiperparâmetros\n","search = GridSearchCV(my_pipeline5, param_grid, scoring='f1', n_jobs=1, cv=cv, refit=True)\n","\n","search.fit(X_train, y_train)"],"metadata":{"id":"XcQQWTsMA1Se"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Média F1: %.3f' % np.mean(search.cv_results_[\"mean_test_score\"]))\n","print('Desvio Padrão F1: %.3f' % np.std(search.cv_results_[\"mean_test_score\"]))"],"metadata":{"id":"-FPqO_HoW6kw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(search.best_params_)"],"metadata":{"id":"3pv7yVVOWnmb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","## Ajustando o desbalanceamento de classes com métodos ensemble\n","\n","Diversas estratégias de aprendizado ensemble foram adaptadas a fim de lidar com problemas de classificação que apresentam desbalanceamento de classes. Abaixo vamos exemplificar o uso do método BalancedRandomForest. Perceba que neste caso, não estamos usando o SVC, pois o próprio algoritmo de aprendizado supervisionado adaptado faz o  tratamento do desbalanceamento."],"metadata":{"id":"WVbEmqbWZNr1"}},{"cell_type":"code","source":["from imblearn.ensemble import BalancedRandomForestClassifier\n","\n","## pipeline específico para os atributos numéricos\n","num_pipeline = Pipeline([\n","    ('imputer', KNNImputer(weights=\"uniform\"))])\n","\n","## pipeline específico para os atributos categóricos\n","cat_pipeline = Pipeline([\n","                         ('imputer', SimpleImputer(strategy='most_frequent')),\n","                         ('encoder', OneHotEncoder(drop='if_binary', sparse=False))])\n","\n","## ColumnTransformer para aplicar cada pipeline ao respectivo tipo de atributo\n","data_pipeline = ColumnTransformer([\n","                                   ('numerical', num_pipeline, num_columns),\n","                                   ('categorical', cat_pipeline, cat_columns)])\n","\n","\n","\n","## pipeline que une as transformações definidas anteriormente e aplica a \n","## normalização em todos os atributos, seguida por BalancedRF\n","my_pipeline6 = imbpipeline(steps = [['data_transform', data_pipeline],\n","                                    ['scaler', MinMaxScaler()],\n","                                    ['balancedRF', BalancedRandomForestClassifier(sampling_strategy=1, max_depth=8,random_state=11)]])\n","\n","\n","# avalia o pipeline\n","scores6 = cross_val_score(my_pipeline6, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)"],"metadata":{"id":"t-s9yNUZZcaO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Média F1: %.3f' % np.mean(scores6))\n","print('Desvio Padrão F1: %.3f' % np.std(scores6))"],"metadata":{"id":"GjWJEG8YaJWI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","\n","## Sua Vez!\n","\n","\n","A atividade autônoma para reforço do conteúdo segue a mesma linha do que foi proposto no notebook anterior, agregando a correção de desbalanceamento de dados (se pertinente). **Relembrando**, os alunos devem:\n","\n","*   Se organizar em grupos de **até 3 alunos** (interessante que já seja o grupo para realização do projeto final da disciplina).\n","*   Escolher um conjunto de dados que **não está pronto** para análise por algoritmos de aprendizado supervisionado, apresentando a **necessidade de limpeza e/ou transformação de dados**. Idealmente, selecionar um conjunto de dados que contenha tipos mistos de atributos e que seja de interesse do grupo para realização do projeto final\n","*   Implementar um pipeline (recomendado o uso de `Pipeline` no scikit-learn) para realizar o pré-processamento de dados utilizando os métodos vistos nesta semana: imputação de valores, codificação de dados categóricos, discretização de dados numéricos, normalização de dados, ajuste de desbalanceamento de classes. Eventualmente nem todos serão necessários nos dados, mas sugere-se tentar usar ao menos dois destes. A imputação de valores pode ser utilizada tanto para corrigir valores faltantes como ruídos/outliers.\n","*   Incluir no pipeline o treinamento de modelos de aprendizado de máquina. Os grupos podem optar por realizar um spot-checking de algoritmos e/ou selecionar um algoritmo específico e realizar a otimização dos seus hiperparâmetros, conforme exemplos deste notebook.\n","*    Executar o pipeline para os dados selecionados, aplicando as etapas de pré-processamento e treinamento de modelos definidas pelo grupo. \n","*    Avaliar o desempenho do modelo final com os dados de teste (evitem data leakage!), reportando matriz de confusão e métrica(s) de desempenho(s) selecionada(s) pelo grupo.\n","\n","Os grupos devem submeter no Moodle o seu notebook do Google Colab exportado em .pdf e .ypnb, devidamente identificado com os nomes dos integrantes do grupo. Sugere-se que, se possível, os grupos incluam no notebook a ser enviado o link para o Google Colab na nuvem, com saídas salvas, a fim de que a professora e monitores(as) possam acessá-lo em caso de dúvidas na avaliação.\n"],"metadata":{"id":"AbSTMZPJy36O"}}]}