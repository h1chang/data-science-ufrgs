{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CD004 - Aula05_atividades.ipynb","provenance":[{"file_id":"1VZhOZqg_9wcJk1PNGrcGfmjnjD33lyFL","timestamp":1655212425787}],"collapsed_sections":[],"toc_visible":true,"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Especialização em Ciência de Dados - INF/UFRGS e SERPRO**\n","### Disciplina CD004 - Metodologia de Aprendizado de Máquina Supervisionado\n","#### *Profa. Mariana Recamonde-Mendoza (mrmendoza@inf.ufrgs.br)*\n","<br> \n","\n","---\n","***Observação:*** *Este notebook é disponibilizado aos alunos como complemento às aulas síncronas e aos slides preparados pela professora. Desta forma, os principais conceitos são apresentados no material teórico fornecido. O objetivo deste notebook é reforçar os conceitos e demonstrar questões práticas no uso de diferentes algoritmos e estratégias de Aprendizado de Máquina.*\n","\n","\n","---\n"],"metadata":{"id":"0FA12O8Tpjmk"}},{"cell_type":"markdown","source":["<br>\n","\n","## **Aula 05** - **Tópico: Pré-processamento de dados. Redução de dimensionalidade**\n","\n","<br>\n","\n","**Objetivo deste notebook**: Explorar estratégias para mitigar o problema de desbalanceamento de classes, complementando o pipeline da aula anterior que realizava pré-processamento dos dados através da imputação de valores e transformação de dados (codificação, discretização, normalização).\n","<br>\n","\n","---\n","\n"],"metadata":{"id":"VDNPuCNO2tpq"}},{"cell_type":"markdown","source":["\n","\n","##**Predição de 'churn' de clientes de serviço de telecomunicação**\n","\n","Os dados que utilizaremos neste notebook se referem a uma empresa de telecomunicações fictícia que forneceu serviços de telefone residencial e Internet para clientes na Califórnia. O conjunto de dados possui informações sobre os serviços para os quais cada cliente se inscreveu (telefone, várias linhas, internet, segurança online, backup online, proteção de dispositivos, suporte técnico e streaming de TV e filmes), informações da conta do cliente (há quanto tempo eles são clientes, contrato, forma de pagamento, cobrança sem papel, cobranças mensais e cobranças totais) e informações demográficas sobre os clientes (sexo, faixa etária, se eles têm parceiros e dependentes). Há, ainda, uma coluna chamada 'churn', que indica os clientes que desistiram do contrato do serviço no último mês. O objetivo da tarefa preditiva é identificar o churn (i.e., saída) de clientes a partir das informações coletadas. Os dados podem ser acessados neste [link](https://www.kaggle.com/datasets/blastchar/telco-customer-churn).\n","\n","Este é um problema de predição que usualmente apresenta **classes desbalanceadas**. Neste notebook vamos adicionar ao pipeline de análise dos dados estratégias para tratar o desbalanceamento de classes."],"metadata":{"id":"DbJNQfaM4ERT"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"jiutqCcGYYM0"}},{"cell_type":"code","source":["## Carregando as bibliotecas básicas necessárias\n","# A primeira linha é incluída para gerar os gráficos logo abaixo dos comandos de plot\n","%matplotlib inline              \n","import pandas as pd             # para análise de dados \n","import matplotlib.pyplot as plt # para visualização de informações\n","import seaborn as sns           # para visualização de informações\n","import numpy as np              # para operações com arrays multidimensionais"],"metadata":{"id":"GIOBsC_-Khtg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Carregando e inspecionando os dados\n","\n","Primeiramente, vamos carregar algumas bibliotecas importantes do Python e os dados a serem utilizados neste estudo. Os dados são disponibilizados através de um link, que também pode ser diretamente acessado pelos alunos."],"metadata":{"id":"dbr-VwMq6OPG"}},{"cell_type":"code","source":["## Bibliotecas para treinamento/avaliação de modelos\n","from sklearn.model_selection import RepeatedKFold, train_test_split, cross_validate, cross_val_score, GridSearchCV\n","from sklearn import metrics\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","sns.set()"],"metadata":{"id":"2lyNg5Sluhbg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Carregando os dados\n","df = pd.read_csv(\"https://drive.google.com/uc?export=view&id=10VrzI8mA2wPvkNIDaLzv-IglPU-Febtf\")#,na_values=\"NA\")\n","df  "],"metadata":{"id":"bPcnFsAB9kv0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Características gerais do dataset\n","print(\"O conjunto de dados possui {} linhas e {} colunas\".format(df.shape[0], df.shape[1]))"],"metadata":{"id":"4BWj00iDltyH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A coluna *'Churn'* contém a classificação de cada instância. Vamos avaliar a distribuição de classes do problema."],"metadata":{"id":"UqoChZTpAEMu"}},{"cell_type":"code","source":["## Distribuição do atributo alvo\n","plt.hist(df['Churn'])\n","plt.title(\"Distribuição do atributo alvo\")\n","plt.show()"],"metadata":{"id":"h0NT9TCiAeLX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Quais os tipos de atributos nos dados? Os dados possuem valores faltantes?\n","Podemos responder estas perguntas utilizando o método .info() para um dataframe."],"metadata":{"id":"ap9Axa7H1fpD"}},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"fwVKHnB1SdOM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["O atributo customerID é único para cada instância e no geral não possui valor preditivo. Iremos removê-lo da análise."],"metadata":{"id":"J_EFTXXhkfXC"}},{"cell_type":"code","source":["customer_ids = df['customerID']\n","df = df.drop(['customerID'], axis=1)"],"metadata":{"id":"QC5YOq8Jkeq2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##remove as duplicatas\n","df=df.drop_duplicates(keep='last')"],"metadata":{"id":"lfnumOq4pdvG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como atributos categóricos e numéricos podem demandar pré-processamentos diferentes, vamos separar os respectivos \"nomes\" em dois vetores distintos, facilitando a manipulação dos dados posteriormente."],"metadata":{"id":"UoOevD6R14tT"}},{"cell_type":"code","source":["## Separa os atributos em vetores, de acordo com o tipo de dado (categórico ou numérico)\n","cat_columns=list(df.drop(['Churn'], axis=1).select_dtypes(include=[\"object\"]).columns)\n","print(cat_columns)\n","\n","num_columns=list(df.select_dtypes(include=[\"int64\", \"float64\"]).columns)\n","print(num_columns)"],"metadata":{"id":"XEkRHdAqSS4Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inspecionando a distribuição dos dados numéricos e categóricos:"],"metadata":{"id":"RhmuEpuh2J8f"}},{"cell_type":"code","source":["def dist_plot(df,columns,type='boxplot'):\n","    plt.figure(figsize=(16, 5))\n","    for indx, var  in enumerate(columns):\n","        plt.subplot(1, 3, indx+1)\n","        if (type=='boxplot'):\n","          g = sns.boxplot(x=var, data=df,showfliers=True)\n","        else: \n","          if (type=='histogram'):\n","            g = sns.histplot(x=var, data=df)\n","    plt.tight_layout()\n","\n","def count_plot(df,columns):\n","    plt.figure(figsize=(20, 12))\n","    for indx, var  in enumerate(columns):\n","        plt.subplot(6, 3, indx+1)\n","        g = sns.countplot(x=var, data=df)\n","    plt.tight_layout()"],"metadata":{"id":"45aavabCVANv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count_plot(df,cat_columns)"],"metadata":{"id":"YJ0cj1hbmIUo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dist_plot(df,num_columns)#,type=\"histogram\")"],"metadata":{"id":"DcfYrcxOlASp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","\n","\n","### Criando conjuntos de treino e teste para avaliação de modelos\n","\n","\n","Antes de iniciar o treinamento do modelo, lembre-se que é recomendado sempre reservar uma porção dos dados para teste, a qual somente será utilizada para avaliação do modelo final (após todo o processo de treinamento e otimização de hiperparâmetros).\n","\n","Vamos fazer esta divisão, separando 20% para teste. Entretanto, primeiro precisamos dividir os dados entre atributos (X) e classe (y). Também iremos codificar as classes Yes/No para 1/0.\n","\n"],"metadata":{"id":"63nQ-E0jVQzW"}},{"cell_type":"code","source":["## Separa o dataset em duas variáveis: os atributos/entradas (X) e a classe/saída (y)\n","X = df.drop(['Churn'], axis=1)\n","y = df['Churn'].values"],"metadata":{"id":"pd1ZSQRnXQq3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Faremos o mapeamento das classes Yes/No para 1/0. Por padrão, as funções de avaliação assumem que a classe 1 é a positiva/de interesse."],"metadata":{"id":"AfWwwt-QiB16"}},{"cell_type":"code","source":["## substitui No' por 0, 'Yes' por 1\n","y = np.array([0 if y=='No' else 1 for y in y]) "],"metadata":{"id":"vXM10ZIZjZzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Faz a divisão entre treino (80%) e teste (20%).\n","## O conjunto de treino representa os dados que serão usados\n","## ao longo do desenvolvimento do modelo\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,stratify=y,random_state=42) "],"metadata":{"id":"eZbsoLfsXkoZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","## Pré-Processamento dos Dados\n","\n","Nesta seção, vamos realizar o pré-processamento dos dados com Pipelines. Em suma, os passos executados para cada tipo de atributo são:\n","\n","*   Atributos **numéricos**: imputação de valores faltantes (com o método KNN) e normalização\n","*   Atributos **categóricos**: imputação de valores faltantes (com a moda), one-hot encoding, e normalização.\n","\n","A normalização será feita pelo método StandardScaler (z-score), mais recomendado para uso com PCA. \n","\n","Salienta-se que como o objetivo deste notebook é observar os resultados dos métodos de redução de dimensionalidade, a aplicação destes métodos e o subsequente treinamento de modelos não será realizado com nested/k-fold cross-validation, mas sim com um simples holdout. Isto facilita a manipulação dos dados e a visualização dos seus resultados. Na prática, os métodos de redução de dimensionalidade devem ser inseridos no Pipeline que será executado a cada iteração do processo de treinamento e validação dos modelos.\n"],"metadata":{"id":"ZkOKOBleLW0b"}},{"cell_type":"code","source":["from sklearn.impute import KNNImputer\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","\n","## pipeline específico para os atributos numéricos\n","num_pipeline = Pipeline([\n","                         ('imputer', KNNImputer(weights=\"uniform\"))])\n","\n","## pipeline específico para os atributos categóricos\n","cat_pipeline = Pipeline([\n","                         ('imputer', SimpleImputer(strategy='most_frequent')),\n","                         ('encoder', OneHotEncoder(drop='if_binary', sparse=False))])\n","\n","## ColumnTransformer para aplicar cada pipeline ao respectivo tipo de atributo\n","data_pipeline = ColumnTransformer([\n","                                   ('numerical', num_pipeline, num_columns),\n","                                   ('categorical', cat_pipeline, cat_columns)])\n","\n","## define pipeline que une as transformações definidas anteriormente e aplica a \n","## normalização com método StandardScaler() em todos os atributos\n","prep_pipeline = Pipeline([\n","                 ('data_transform', data_pipeline),\n","                 ('data_normalize',StandardScaler())])\n","\n","## ajusta o pipeline a partir dos dados de treino, e na sequência aplica em \n","## treino e teste separadamente\n","prep_pipeline.fit(X_train)\n","X_train_prep = prep_pipeline.transform(X_train)\n","X_test_prep = prep_pipeline.transform(X_test)"],"metadata":{"id":"cKincUm4BBbT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ajusta nome das columas e mostra dataframe após pré-processamento\n","columns = np.append(num_columns,prep_pipeline[0].named_transformers_['categorical']['encoder'].get_feature_names_out(cat_columns))\n","df_train_prep = pd.DataFrame(X_train_prep, columns=columns)\n","df_test_prep = pd.DataFrame(X_test_prep, columns=columns)"],"metadata":{"id":"yfd_tvRj_iY3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualizando o formato do conjunto de dados de treinamento após aplicação do Pipeline de pré-processamento de dados."],"metadata":{"id":"GVoCW6jfGO3J"}},{"cell_type":"code","source":["df_train_prep"],"metadata":{"id":"_TOF0mezBNXD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Características gerais do dataset após pré-processamento\n","print(\"O conjunto de dados possui {} linhas e {} colunas referentes a atributos\".format(df_train_prep.shape[0], df_train_prep.shape[1]))"],"metadata":{"id":"ya6E2cUAGgge"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","## Redução de dimensionalidade com PCA"],"metadata":{"id":"5YZlFWYMGWJc"}},{"cell_type":"markdown","source":["A aplicação do método PCA tem duas grandes utilidades com aprendizado supervisionado: auxiliar na visualização dos dados e reduzir a dimensionalidade dos dados para acelerar ou facilitar o treinamento de modelos preditivos."],"metadata":{"id":"DcbZq6g0GtLe"}},{"cell_type":"markdown","source":["### Compreendendo a saída do PCA"],"metadata":{"id":"pyTrh9WKKSr0"}},{"cell_type":"markdown","source":["Nas células abaixo, vamos explorar o uso de PCA nos nossos dados sobre *Churn* de clientes. Os gráficos abaixo irão aplicar o PCA aos dados do treinamento, retornando as informações sobre a variância explicada pelas componentes principais. Execute as células, observe os resultados, e responda as perguntas a seguir.\n"],"metadata":{"id":"TboKVxcrG69K"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","\n","#define modelo PCA a usar, sem restringir número de componentes principais para manter\n","pca = PCA()\n","\n","#ajusta o modelo PCA aos dados (sem modificar os dados)\n","pca.fit(X_train_prep)\n","\n","plt.figure(figsize=(10, 5))\n","PC_values = np.arange(pca.n_components_) + 1\n","plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n","plt.title('Scree Plot')\n","plt.xlabel('Principal Component')\n","plt.ylabel('Variance Explained')\n","plt.show()"],"metadata":{"id":"nNKDCGZXDo60"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## descomente se quiser observar os valores das variâncias das componentes 1-11\n","print(pca.explained_variance_ratio_[0:10])"],"metadata":{"id":"WPHs6hgVIAqt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 5))\n","PC_values = np.arange(pca.n_components_) + 1\n","plt.plot(PC_values, np.cumsum(pca.explained_variance_ratio_), 'o-', linewidth=2, color='blue')\n","plt.title('Cumulative Variance Explained')\n","plt.xlabel('Principal Component')\n","plt.ylabel('Cumulative Variance Explained')\n","plt.show()"],"metadata":{"id":"-f-z6J2eEu_w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## descomente se quiser observar os valores das variâncias cumulativas explicadas \n","## pelas componentes 1-11\n","print(np.cumsum(pca.explained_variance_ratio_)[0:16])"],"metadata":{"id":"6AqnEC9_IB4d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**(1) Responda >>>** (i) Quanto da variância dos dados é explicado individualmente pela primeira e pela segunda componente? (ii) Quantas componentes são necessárias para explicar em torno de 90% da variância dos dados?\n"],"metadata":{"id":"3Gi7azxKIbQc"}},{"cell_type":"markdown","source":["> ***Sua resposta aqui:***\n","\n","(i) O primeiro componente PC1 captura 0.27949207 da variância nos dados, e o segundo componente PC2 captura 0.14413663 da variância.\n","\n","(ii) Observando a variância acumulada dos componentes até o componente 16, temos a variância acumulada de 0.91792999."],"metadata":{"id":"Z5aCQJkDIglY"}},{"cell_type":"markdown","source":["### Visualizando os dados com PCA"],"metadata":{"id":"PV6deZv-KnwY"}},{"cell_type":"code","source":["#projetando os dados de 40 dimensões para 2 (PC1 e PC2)\n","pca = PCA(2) \n","X_train_projected = pca.fit_transform(X_train_prep)\n","X_test_projected = pca.transform(X_test_prep)\n","print(X_train_prep.shape)\n","print(X_train_projected.shape)"],"metadata":{"id":"ylaucFG1K8ZM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots()\n","scatter = ax.scatter(X_train_projected[:, 0], X_train_projected[:, 1],\n","            c=y_train, edgecolor='none', alpha=0.5,\n","            cmap=plt.cm.get_cmap('Dark2', 2))\n","\n","legend1 = ax.legend(*scatter.legend_elements(),title=\"Classes\")\n","ax.add_artist(legend1)\n","\n","plt.xlabel('component 1')\n","plt.ylabel('component 2')\n"],"metadata":{"id":"Nd8UnehCEDwh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**(2) Responda >>>** (i) A projeção dos dados de acordo com as duas componentes principais apresenta algum tipo de agrupamento? Se sim, relate quantos grupos/clusters foram observado. (ii) Caso observe a formação de grupos, discuta se os grupos parecem ser divididos de acordo com a classe do problema (0/1) ou se não é esta característica que predomina na diferença entre os grupos.\n"],"metadata":{"id":"Mn6TWqANQxQF"}},{"cell_type":"markdown","source":["> ***Sua resposta aqui:***\n","(i) Sim, foram observados 2 grupos um maior a esquerda do gráfico e um menor a direita.\n","\n","(ii) Os grupos não parecem separar completamente as classes do problema (churn = 0 ou 1). Apesar do grupo menor visualmente apresentar poucos casos da classe 1, isso pode ser resultado do desbalanceamento das classes."],"metadata":{"id":"v9ihIPYjQ0AK"}},{"cell_type":"markdown","source":["O PCA resulta de uma combinação linear entre os atributos originais. Assim, podemos observar qual o coeficiente associado com cada atributo para formar cada uma das componentes principais. A interpretação dos componentes principais baseia-se em descobrir quais atributos são mais fortemente correlacionadas com cada componente, ou seja, quais desses atributos possuem coeficientes de maior magnitude, em qualquer direção (positivo ou negativo), na componente analisada. Quanto maior o valor do coeficiente, mais importante ele é para o cálculo da componente. A decisão sobre que valor de coeficiente é grande ou pequeno é uma decisão subjetiva, por isso, uma análise relativa (entre atributos para uma mesma componente) pode ser mais útil em um primeiro momento.\n","\n","Nas células a seguir, vamos extrair os coeficientes dos atributos no nosso conjunto de dados para cada uma das duas componentes principais. Em seguida, vamos visualizar os valores através de um heatmap (mapa de calor), facilitando a comparação entre atributos, e a identificação de diferentes características mais relevantes para cada componente"],"metadata":{"id":"di4W86o2U-vZ"}},{"cell_type":"code","source":["loadings = pca.components_\n","num_pc = pca.n_features_\n","pc_list = [\"PC\"+str(i) for i in list(range(1, num_pc+1))]\n","loadings_df = pd.DataFrame.from_dict(dict(zip(pc_list, loadings)))\n","loadings_df['variable'] = df_train_prep.columns.values\n","loadings_df = loadings_df.set_index('variable')\n","loadings_df #descomente se quiser visualizar os dados brutos"],"metadata":{"id":"lOaqvn0dTfph"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 10))\n","sns.heatmap(loadings_df, annot=True, cmap='Spectral')\n","plt.show()"],"metadata":{"id":"s6uX5zWMUHKw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**(3) Responda >>>** Observe o heatmap e relate: (i) que atributos apresentaram maiores coeficientes para a PC1 e para a PC2? (ii) com base nesta análise, você consegue levantar alguma hipótese sobre a principal característica capturada pela PC1 e que pode explicar o resultado da visualização dos dados com base na PC1 e na PC2 descrito na questão (2)?"],"metadata":{"id":"DOeIp5uVXRdC"}},{"cell_type":"markdown","source":["> ***Sua resposta aqui:***\n","\n","(i)\n","\n","Atributos com maiores correlação positiva ou negativa com atributo PC1 são: [InternetService_No\n","OnlineSecurity_No internet service\n","OnlineBackup_No internet service\n","DeviceProtection_No internet service\n","TechSupport_No internet service\n","StreamingTV_No internet service\n","StreamingMovies_No internet service\n","MonthlyCharges].\n","\n","Atributos com maiores correlação positiva ou negativa com atributo PC2 são: [Contract_Two year,\n","tenure,\n","Contract_Month-to-month, TotalCharges].\n","\n","(ii)\n","\n","Os atributos [InternetService, OnlineSecurity, OnlineBackup, DeviceProtection\n","TechSupport, StreamingTV, StreamingMovies, MonthlyCharges] quando tem a classe  'No internet service' apresentam alta correlaçao positiva com o componente PC1.\n","Que pode explicar a separação dos dois grupos da questão (2), visto que os dois grupos eram separados no eixo x (PC1).\n","Esses mesmos atributos com os valores 'No' apresentam correlação negativa com o componente PC2."],"metadata":{"id":"O_49JhnDXU7M"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","## Treinando um modelo preditivo a partir das componentes principais"],"metadata":{"id":"WF4gontfYPA6"}},{"cell_type":"markdown","source":["Nesta seção, vamos comparar o desempenho de modelos preditivos treinados com os dados originais, e com as componentes derivadas do PCA. Utilizaremos as duas componentes principais (PC1 e PC2) e posteriormente as 10 componentes principais (que explicam em torno de 80% da variância nos dados). Utilizaremos como classificador base um SVM com kernel radial e termo de regularização C=0.1 e não abordaremos a tarefa de otimizar os hiperparâmetros do modelo, utilizando o holdout simples (de 2 vias) para fins de comparação de desempenho. Salienta-se que ambos os conjuntos de treino e teste já passaram pelos mesmos passos de pré-processamento, incluindo redução de dimensionalidade com PCA."],"metadata":{"id":"oPT3VnnCYTj3"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","\n","## treinando e avaliando com os dados originais\n","clf_original = SVC(kernel='rbf', C=0.1)\n","clf_original = clf_original.fit(X_train_prep,y_train)\n","clf_original_pred = clf_original.predict(X_test_prep)\n","\n","## treinando e avaliando com duas componentes principais\n","clf_pca2 = SVC(kernel='rbf', C=0.1)\n","clf_pca2 = clf_pca2.fit(X_train_projected,y_train)\n","clf_pca2_pred = clf_pca2.predict(X_test_projected)\n","\n","## fazendo a análise do PCA, extraindo 10 componentes principais\n","## usadas no treinamento e avaliação\n","pca10 = PCA(10) \n","X_train_projected10 = pca10.fit_transform(X_train_prep)\n","X_test_projected10 = pca10.transform(X_test_prep)\n","clf_pca10 = SVC(kernel='rbf', C=0.1)\n","clf_pca10 = clf_pca10.fit(X_train_projected10,y_train)\n","clf_pca10_pred = clf_pca10.predict(X_test_projected10)\n"],"metadata":{"id":"1fwFqn-xZGWw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, recall_score, precision_score,f1_score,ConfusionMatrixDisplay\n","\n","cm = confusion_matrix(y_test, clf_original_pred, labels=clf_original.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf_original.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","print(\"F1: {}\".format(round(f1_score(y_test, clf_original_pred),3)))\n","print(\"Recall: {} \".format(round(recall_score(y_test, clf_original_pred),3)))\n","print(\"Precision: {}\".format(round(precision_score(y_test, clf_original_pred),3)))"],"metadata":{"id":"Lw3iAtXlZ9lB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cm = confusion_matrix(y_test, clf_pca2_pred, labels=clf_original.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf_original.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","print(\"F1: {}\".format(round(f1_score(y_test, clf_pca2_pred),3)))\n","print(\"Recall: {} \".format(round(recall_score(y_test, clf_pca2_pred),3)))\n","print(\"Precision: {}\".format(round(precision_score(y_test, clf_pca2_pred),3)))"],"metadata":{"id":"dk9Lfcn3ntgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cm = confusion_matrix(y_test, clf_pca10_pred, labels=clf_pca10.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf_pca10.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","print(\"F1: {}\".format(round(f1_score(y_test, clf_pca10_pred),3)))\n","print(\"Recall: {} \".format(round(recall_score(y_test, clf_pca10_pred),3)))\n","print(\"Precision: {}\".format(round(precision_score(y_test, clf_pca10_pred),3)))"],"metadata":{"id":"RDQmKWS2n5PR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**(4) Responda >>>** Qual o desempenho dos modelos SVM treinados sem e com PCA? Qual foi o impacto do uso do PCA sobre o desempenho do modelo (neste cenário) e como o aumento do número de componentes variou o desempenho?"],"metadata":{"id":"HVvRUiwxq1SE"}},{"cell_type":"markdown","source":["> ***Sua resposta aqui:*** O desempenho dos modelos SVM com PCA teve resultados próximos, mas não tão bons quanto o modelo sem PCA. Mas houve melhora nas métricas com o aumento no número de componentes de 2 para 10."],"metadata":{"id":"qYtszdJErRza"}},{"cell_type":"markdown","source":["Salienta-se que a resposta para a pergunta acima não necessariamente se aplica a outros problemas de predição. Isto é, a resposta é válida para o problema e para os dados analisados, e as observações podem variar para outros conjuntos de dados."],"metadata":{"id":"ikUUWAX-6p4_"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","## Redução de dimensionalidade com seleção de atributos por Filtro"],"metadata":{"id":"am_irQPgrWBp"}},{"cell_type":"markdown","source":["O módulo [sklearn.feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html) possui uma série de funções para selecionar atributos, removendo aqueles que parecem não ser relevantes para uma determinada tarefa preditiva. As seções a seguir avaliam as estratégias baseadas em filtro usando o método SelectKBest(). Este método aplica um critério para ordenação dos atributos de acordo com sua relevância ou poder discriminativo e então retém apenas um subconjunto dos mais relevantes, de acordo com valor 'k' informado na chamada ao método."],"metadata":{"id":"sG0otCr5rzYv"}},{"cell_type":"code","source":["from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_classif\n","\n","## define o método de seleção de atributos (SelectKBest), critério para ordenar\n","## os atributos (F_classif),e  número de atributos para selecionar (2)\n","## Em seguida faz ajustae nos dados e transforma treino e teste\n","fsel_fclassif = SelectKBest(f_classif, k=2)\n","X_train_fsc = fsel_fclassif.fit_transform(X_train_prep, y_train)\n","X_test_fsc = fsel_fclassif.transform(X_test_prep)\n","print(X_train_fsc)\n","print(\"O conjunto de dados possui {} linhas e {} colunas\".format(X_train_fsc.shape[0], X_train_fsc.shape[1]))\n","\n","## encontra os índices das colunas de atributos selecionados\n","cols = fsel_fclassif.get_support(indices=True)\n","features_df_new = df_train_prep.iloc[:,cols]\n","print(\"Atributos selecionados: \")\n","print(columns[cols])"],"metadata":{"id":"blfcsf4bu10p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf_featSel_Fclassif = SVC(kernel='rbf', C=0.1)\n","clf_featSel_Fclassif = clf_featSel_Fclassif.fit(X_train_fsc,y_train)\n","clf_featSel_Fclassif_pred = clf_featSel_Fclassif.predict(X_test_fsc)\n","\n","cm = confusion_matrix(y_test, clf_featSel_Fclassif_pred, labels=clf_featSel_Fclassif.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf_featSel_Fclassif.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","print(\"F1: {}\".format(round(f1_score(y_test, clf_featSel_Fclassif_pred),3)))\n","print(\"Recall: {} \".format(round(recall_score(y_test, clf_featSel_Fclassif_pred),3)))\n","print(\"Precision: {}\".format(round(precision_score(y_test, clf_featSel_Fclassif_pred),3)))"],"metadata":{"id":"GH1H5uQsy5I0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**(5) Responda >>>** (i) Quais foram os top 2 atributos selecionados pelo método ANOVA F-value (F-Classif)? (ii) Qual foi o desempenho do modelo treinado com os 2 melhores atributos de acordo com o método F-Classif? (iii) Comparando recall e precisão, os valores obtidos com este modelo utilizando seleção de atributos (e top 2 atributos) foram acima ou abaixo do desempenho obtido com o modelo que usa apenas as duas primeiras componentes do PCA?"],"metadata":{"id":"hfETEQVa09kY"}},{"cell_type":"markdown","source":["> ***Sua resposta aqui:***\n","\n","(i) Os dois atributos selecionados foram: ['OnlineSecurity_No' 'Contract_Month-to-month'].\n","\n","(ii) \n","\n","Resultado com seleção de atributos:\n","\n","*   F1: 0.575\n","*   Recall: 0.704 \n","*   Precision: 0.485\n","\n","Resultado dos top 2 atributos PCA:\n","\n","*   F1: 0.454\n","*   Recall: 0.382 \n","*   Precision: 0.561\n","\n","Os resultados com a seleção de algoritmo ANOVA F-value (F-Classif) foram superiores para as métricas de F1 e Recall.\n","\n","\n"],"metadata":{"id":"E2hq8KJk1XG-"}},{"cell_type":"markdown","source":["Para fins de comparação, podemos verificar quais são os top 10 atributos de acordo com o método `f_classif` e utilizar somente estes para treinamento de um modelo."],"metadata":{"id":"WTUlOH5K2N50"}},{"cell_type":"code","source":["fsel_fclassif10 = SelectKBest(f_classif, k=10)\n","X_train_fsc10 = fsel_fclassif10.fit_transform(X_train_prep, y_train)\n","X_test_fsc10 = fsel_fclassif10.transform(X_test_prep)\n","#print(X_train_fsc10)\n","print(\"O conjunto de dados possui {} linhas e {} colunas\".format(X_train_fsc10.shape[0], X_train_fsc10.shape[1]))\n","\n","cols = fsel_fclassif10.get_support(indices=True)\n","features_df_new = df_train_prep.iloc[:,cols]\n","print(\"Atributos selecionados: \")\n","print(columns[cols])"],"metadata":{"id":"S8gdCdxUzrHQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf_featSel_Fclassif10 = SVC(kernel='rbf', C=0.1)\n","clf_featSel_Fclassif10 = clf_featSel_Fclassif10.fit(X_train_fsc10,y_train)\n","clf_featSel_Fclassif10_pred = clf_featSel_Fclassif10.predict(X_test_fsc10)\n","\n","cm = confusion_matrix(y_test, clf_featSel_Fclassif10_pred, labels=clf_featSel_Fclassif10.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf_featSel_Fclassif10.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","print(\"F1: {}\".format(round(f1_score(y_test, clf_featSel_Fclassif10_pred),3)))\n","print(\"Recall: {} \".format(round(recall_score(y_test, clf_featSel_Fclassif10_pred),3)))\n","print(\"Precision: {}\".format(round(precision_score(y_test, clf_featSel_Fclassif10_pred),3)))"],"metadata":{"id":"57Bzi5_cz6Yw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Podemos observar desempenhos diferentes entre um modelo treinado a partir das 10 componentes principais obtidas pelo PCA e a partir dos top 10 atributos selecionados com um método de filtro (F-classif)."],"metadata":{"id":"VMhCoGAP24Ty"}},{"cell_type":"markdown","source":["**(6) Responda >>>** Copie os códigos acima da seleção de atributos com SelectKBest e faça modificações para utilizar o método mutual_info_classif (ao invés de f_classif) para avaliação e priorização de atributos a serem selecionados. Treine e avalie o modelo para 2 e 10 atributos. Após a conclusão da implementação, execute o código e comente na sua resposta sobre: i) como o desempenho destes modelos se compara aos modelos com seleção de atributos com SelectKBest e método F-Classif e ii) quais atributos foram selecionados em cada casa e se ocorreu diferenças entre os métodos ou não quanto ao top K atributos  (para cada tamanho de subconjunto)"],"metadata":{"id":"7Z-zrm-C3NHA"}},{"cell_type":"code","source":["from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import mutual_info_classif"],"metadata":{"id":"wrZL8EFN31fU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## >>> Sua resposta para a implementação \n","\n","# fsel_mutualclassif2\n","fsel_mutualclassif2 = SelectKBest(mutual_info_classif, k=2)\n","X_train_fsmutualclassif2 = fsel_mutualclassif2.fit_transform(X_train_prep, y_train)\n","X_test_fsmutualclassif2 = fsel_mutualclassif2.transform(X_test_prep)\n","print(\"O conjunto de dados possui {} linhas e {} colunas\".format(X_train_fsmutualclassif2.shape[0], X_train_fsmutualclassif2.shape[1]))\n","\n","cols = fsel_mutualclassif2.get_support(indices=True)\n","features_df_new = df_train_prep.iloc[:,cols]\n","print(\"Atributos selecionados: \")\n","print(columns[cols])\n","\n","\n","# fsel_mutualclassif10\n","fsel_mutualclassif10 = SelectKBest(mutual_info_classif, k=10)\n","X_train_fsmutualclassif10 = fsel_mutualclassif10.fit_transform(X_train_prep, y_train)\n","X_test_fsmutualclassif10 = fsel_mutualclassif10.transform(X_test_prep)\n","print(\"O conjunto de dados possui {} linhas e {} colunas\".format(X_train_fsmutualclassif10.shape[0], X_train_fsmutualclassif10.shape[1]))\n","\n","cols = fsel_mutualclassif10.get_support(indices=True)\n","features_df_new = df_train_prep.iloc[:,cols]\n","print(\"Atributos selecionados: \")\n","print(columns[cols])"],"metadata":{"id":"DyLtkqFT5Ch5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Matriz de confusão para mutual_info_classif k = 2:')\n","\n","clf_featSel_Mclassif2 = SVC(kernel='rbf', C=0.1)\n","clf_featSel_Mclassif2 = clf_featSel_Mclassif2.fit(X_train_fsmutualclassif2,y_train)\n","clf_featSel_Mclassif2_pred = clf_featSel_Mclassif2.predict(X_test_fsmutualclassif2)\n","\n","cm = confusion_matrix(y_test, clf_featSel_Mclassif2_pred, labels=clf_featSel_Mclassif2.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf_featSel_Mclassif2.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","print(\"F1: {}\".format(round(f1_score(y_test, clf_featSel_Mclassif2_pred),3)))\n","print(\"Recall: {} \".format(round(recall_score(y_test, clf_featSel_Mclassif2_pred),3)))\n","print(\"Precision: {}\".format(round(precision_score(y_test, clf_featSel_Mclassif2_pred),3)))\n","\n","print('Matriz de confusão para mutual_info_classif k = 10:')\n","\n","clf_featSel_Mclassif10 = SVC(kernel='rbf', C=0.1)\n","clf_featSel_Mclassif10 = clf_featSel_Mclassif10.fit(X_train_fsmutualclassif10,y_train)\n","clf_featSel_Mclassif10_pred = clf_featSel_Mclassif10.predict(X_test_fsmutualclassif10)\n","\n","cm = confusion_matrix(y_test, clf_featSel_Mclassif10_pred, labels=clf_featSel_Mclassif10.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf_featSel_Mclassif10.classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","print(\"F1: {}\".format(round(f1_score(y_test, clf_featSel_Mclassif10_pred),3)))\n","print(\"Recall: {} \".format(round(recall_score(y_test, clf_featSel_Mclassif10_pred),3)))\n","print(\"Precision: {}\".format(round(precision_score(y_test, clf_featSel_Mclassif10_pred),3)))"],"metadata":{"id":"0Tm3d2cQdIxQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> ***Sua resposta aqui (para a interpretação dos resultados):***\n","\n","(i)\n","\n","Resultados para seleção de atributos com mutual_info_classif\n","\n","*   k = 2: [F1: 0.505, Recall: 0.484, Precision: 0.528]\n","*   k = 10: [F1: 0.517, Recall: 0.441, Precision: 0.626]\n","\n","Resultados para seleção de atributos com f_classif\n","*   k = 2: [F1: 0.575, Recall: 0.704, Precision: 0.485]\n","*   k = 10: [F1: 0.521, Recall: 0.441, Precision: 0.636]\n","\n","Os dois métodos de escolha de atributos apresentaram uma diferença considerável para o caso k = 2, e uma diferença muito pequena no caso k = 10. Isso pode ser explicada analisando os atributos selecionados, como veremos a seguir.\n","\n","ii) \n","\n","Atributos selecionados com mutual_info_classif:\n","*   top 2: ['tenure' 'Contract_Month-to-month']\n","*   top 10: ['tenure' 'MonthlyCharges' 'TotalCharges' 'InternetService_Fiber optic' 'OnlineSecurity_No' 'OnlineSecurity_No internet service' 'TechSupport_No'\n"," 'Contract_Month-to-month' 'Contract_Two year' 'PaymentMethod_Electronic check']\n","\n","Atributos selecionados com f_classif:\n","*   top 2: ['OnlineSecurity_No' 'Contract_Month-to-month']\n","*   top 10: ['tenure' 'InternetService_Fiber optic' 'OnlineSecurity_No'\n"," 'OnlineBackup_No' 'DeviceProtection_No' 'TechSupport_No'\n"," 'StreamingMovies_No internet service' 'Contract_Month-to-month'\n"," 'Contract_Two year' 'PaymentMethod_Electronic check']\n","\n"," Para o top 1, houve diferença de 1 dos elementos o que explica a diferença de performance dos dois modelos.\n"," Enquanto que no top 10, essa diferença foi só de 3 atributos dos 10 selecionados.\n"," É possível perceber que a performance do modelo varia mais quando poucos atributos são selecionados, que a diferença diminui a medida que esse grupo cresce."],"metadata":{"id":"LKcWc_-m8IaV"}}]}