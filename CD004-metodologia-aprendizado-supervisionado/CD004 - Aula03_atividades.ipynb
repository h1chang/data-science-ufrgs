{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CD004 - Aula03_atividades.ipynb","provenance":[{"file_id":"1X_xrXDBkVkBOvtch1XBooI3S6ioQbCdM","timestamp":1654865674640}],"collapsed_sections":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Especialização em Ciência de Dados - INF/UFRGS e SERPRO**\n","### Disciplina CD004 - Metodologia de Aprendizado de Máquina Supervisionado\n","#### *Profa. Mariana Recamonde-Mendoza (mrmendoza@inf.ufrgs.br)*\n","<br> \n","\n","---\n","***Observação:*** *Este notebook é disponibilizado aos alunos como complemento às aulas síncronas e aos slides preparados pela professora. Desta forma, os principais conceitos são apresentados no material teórico fornecido. O objetivo deste notebook é reforçar os conceitos e demonstrar questões práticas no uso de diferentes algoritmos e estratégias de Aprendizado de Máquina.*\n","\n","\n","---\n"],"metadata":{"id":"0FA12O8Tpjmk"}},{"cell_type":"markdown","source":["<br>\n","\n","## **Aula 03** - **Tópico: Pré-processamento de dados. Imputação e Transformação.**\n","\n","<br>\n","\n","**Objetivo deste notebook**: Explorar estratégias de pré-processamento de dados, incluindo imputação de valores e transformação de dados (codificação, discretização, normalização). Conhecer e compreender a utilidade de Pipelines em scikit-learn/Python.\n","<br>\n","\n","---\n","\n"],"metadata":{"id":"VDNPuCNO2tpq"}},{"cell_type":"markdown","source":["\n","##**Exemplos simples de imputação, codificação, discretização de dados com Python**\n","Primeiramente, vamos exemplificar o funcionamento básico dos métodos do scikit-learn voltados para a imputação de dados e transformação de dados. É perceptível a semelhança com o uso dos métodos para normalização de dados que temos explorado nas nossas aulas. A seguir revisamos um exemplo simples de normalização de dados numéricos com StandardScaler() e MinMaxScaler(). "],"metadata":{"id":"5OG6yjU5UO1D"}},{"cell_type":"markdown","source":["### Revisando a normalização de dados"],"metadata":{"id":"IfryK6E0XBTO"}},{"cell_type":"code","source":["## Carregando as bibliotecas básicas necessárias\n","# A primeira linha é incluída para gerar os gráficos logo abaixo dos comandos de plot\n","%matplotlib inline              \n","import pandas as pd             # para análise de dados \n","import matplotlib.pyplot as plt # para visualização de informações\n","import seaborn as sns           # para visualização de informações\n","import numpy as np              # para operações com arrays multidimensionais"],"metadata":{"id":"hSTPGleyVEAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler,MinMaxScaler\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","\n","## gera um conjunto de dados sintético para classificação\n","## o conjunto de dados contém 40 instâncias e 5 atributos\n","## o problema de classificação é binário\n","X, y = make_classification(n_samples=40,n_features=5,random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,test_size=0.2)\n","\n","## O scikit-learn disponibiliza diferentes métodos para normalização/padronização\n","#scaler = StandardScaler()\n","scaler = MinMaxScaler()\n","\n","\n","## para os dados de treino podemos primeiro chamar .fit() e depois .transform()\n","## ou podemos substituir ambas por uma única chamada .fit_transform().\n","## CUIDADO: fit_transform() NÂO deve ser usados para dados de validação/teste\n","X_train_scaled = scaler.fit_transform(X_train) \n","X_test_scaled = scaler.transform(X_test)"],"metadata":{"id":"Yarg1FKxU3pf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_test)"],"metadata":{"id":"ap7-xhTEWnuU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_test_scaled)"],"metadata":{"id":"cTgpmmRgWsKp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Imputação de valores faltantes"],"metadata":{"id":"FSRjTZ_eXEPP"}},{"cell_type":"markdown","source":["Embora as células a seguir lidem com a imputação para valores faltantes, podemos utilizar as mesmas estratégias de imputação para corrigir ruídos/outliers."],"metadata":{"id":"QzHp0drcch02"}},{"cell_type":"code","source":["## fazemos uma cópia dos dados para poder simular valores faltantes\n","X_train_missing = X_train.copy()\n","X_test_missing = X_test.copy()"],"metadata":{"id":"XkTjbuoiZDlD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","## simula alguns valores faltantes, alterando posições aleatórias dos conjuntos\n","## de treinamento e teste\n","\n","X1=random.sample(range(X_train.shape[0]), 8)\n","X2=random.sample(range(X_train.shape[0]), 5)\n","print(X1)\n","print(X2)\n","\n","X_train_missing[X1,0] = np.nan\n","X_train_missing[X2,3] = np.nan\n","\n","\n","X3=random.sample(range(X_test.shape[0]), 2)\n","X4=random.sample(range(X_test.shape[0]), 1)\n","print(X3)\n","print(X4)\n","\n","X_test_missing[X3,0] = np.nan\n","X_test_missing[X4,3] = np.nan"],"metadata":{"id":"dqllaYdXXNDK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## verificando o conjunto de teste após simulação de valores faltantes\n","X_test_missing"],"metadata":{"id":"cB96wapmZXL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## realização a imputação de valores com SimpleImputer\n","## nos atributos (todos numéricos)\n","from sklearn.impute import SimpleImputer\n","\n","## Inicializa um SimpleImputer que irá substituir valores faltantes pela média dos dados\n","imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n","\n","## Estima o parâmetro (média) a partir dos dados de treino e transforma os dados de treino\n","X_train_missing_fix1= imp_mean.fit_transform(X_train_missing)\n","\n","## Transforma os dados de teste\n","X_test_missing_fix1 = imp_mean.transform(X_test_missing)\n","print(X_test_missing_fix1)"],"metadata":{"id":"JpG4zpMJagpE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.impute import KNNImputer\n","\n","\n","## Inicializa um KNNImputer que irá substituir valores faltantes pelo valor estimado com um KNN\n","imp_knn = KNNImputer(n_neighbors=2, weights=\"uniform\")\n","\n","## Estima o parâmetro a partir dos dados de treino e transforma os dados de treino\n","X_train_missing_fix2 = imp_knn.fit_transform(X_train_missing)\n","\n","## Transforma os dados de teste\n","X_test_missing_fix2  = imp_knn.transform(X_test_missing)\n","print(X_test_missing_fix2)"],"metadata":{"id":"u4--VArPcDDP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Para valores categóricos, podemos usar o SimpleImputer com \n","## a moda (most_frequent) ou valor constante\n","df1 = pd.DataFrame([[\"a\", \"x\"],\n","                  [np.nan, \"y\"],\n","                  [\"a\", np.nan],\n","                  [\"b\", \"y\"]], dtype=\"category\")\n","print(df1)\n","\n","## Imputação com a moda\n","imp_mode = SimpleImputer(strategy=\"most_frequent\")\n","print(imp_mode.fit_transform(df1))\n","\n","\n","## Imputação com um valor padrão\n","imp_mode = SimpleImputer(strategy=\"constant\",fill_value='?')\n","print(imp_mode.fit_transform(df1))"],"metadata":{"id":"c84pW1RWbFWD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Discretização de dados numéricos"],"metadata":{"id":"41PM9dVCcqSp"}},{"cell_type":"markdown","source":["As células abaixo demonstram diferentes estratégias para discretizar dados numéricos ('quantile' e 'uniform'), bem como diferentes formas de codificar o resultado da discretização ('ordinal' e 'onehot-dense')."],"metadata":{"id":"8FFFniJ4fO0F"}},{"cell_type":"code","source":["from sklearn.preprocessing import KBinsDiscretizer\n","discr_uni = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n","#discr_uni = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n","#discr_uni = KBinsDiscretizer(n_bins=3, encode='onehot-dense', strategy='uniform')\n","\n","## Faz uma cópia dos dados originais de treino e teste\n","X_train_cp1 = X_train.copy()\n","X_test_cp1 = X_test.copy()\n","\n","## Estima os parâmetros da discretização de dados a partir dos dados de treino e transforma os dados de treino\n","X_train_cp1 = discr_uni.fit_transform(X_train)\n","\n","## Transforma os dados de teste\n","X_test_cp1 = discr_uni.transform(X_test)"],"metadata":{"id":"eoxnG7AeceGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_test_cp1)"],"metadata":{"id":"7gkZF3pedTBb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Codificação de dados categóricos\n","\n","> Bloco com recuo\n","\n"],"metadata":{"id":"0wLpu3_hfkBI"}},{"cell_type":"markdown","source":["A codificação (encoding) de valores categóricos consiste em transformá-los para uma representação numérica. Essa transformação é muito útil para lidar com dados que possuam tipos mistos de atributos, bem como para utilizar algoritmos que possuam a restrição de aceitar como entrada apenas valores numéricos."],"metadata":{"id":"klhWZQxg0hIf"}},{"cell_type":"code","source":["## Simulando exemplos categóricos para análise\n","X1 = np.array([['dog'] * 5 + ['cat'] * 10 + ['rabbit'] * 7 + ['snake'] * 3], dtype=object).T\n","X2 = np.array([['male'] * 2 + ['female'] * 10 + ['male'] * 8 + ['female'] * 5], dtype=object).T\n","\n","X_train_cat = np.concatenate((X1,X2), axis=1)\n","X_test_cat = np.array([[ 'dog', 'male'],\n","                      [ 'cat',  'female'],\n","                      [ 'rabbit', 'male']])"],"metadata":{"id":"uxDTfZymfwdW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train_cat)"],"metadata":{"id":"cZ8awXotf3c3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_test_cat)"],"metadata":{"id":"qqCCuSCLhF5U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exemplo do uso de OrdinalEncoder(). Cada categoria é mapeada para um valor inteiro, de 0 até [número categorias - 1]."],"metadata":{"id":"IXTIGrAKhc2E"}},{"cell_type":"code","source":["from sklearn.preprocessing import OrdinalEncoder\n","enc_ord = OrdinalEncoder()\n","\n","X_train_cat_ord = enc_ord.fit_transform(X_train_cat)\n","X_test_cat_ord  = enc_ord.transform(X_test_cat)"],"metadata":{"id":"EnPrBubQhNb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_test_cat_ord)"],"metadata":{"id":"Ye-PUa8Ch1LI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tendo em vista que o OrdinalEncoder() substitui cada valor categórico por um inteiro, não é recomendado para atributos que sejam nominais, isto é, que não possuem relação de ordem implícita. Para atributos categóricos nominais, é mais recomendado utilizar OneHotEncoder(). A opção Sparse=False determina que será gerado um array com 0's e 1's: cada categoria possível para um determinado atributo vira um novo atributo binário.\n"],"metadata":{"id":"bAW9pNeyh9HZ"}},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","enc_ohe = OneHotEncoder(sparse=False)\n","\n","X_train_cat_ohe = enc_ohe.fit_transform(X_train_cat)\n","X_test_cat_ohe = enc_ohe.transform(X_test_cat)"],"metadata":{"id":"KhPP3GLNh8S4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## visualiza o resultado da codificação \n","print(X_test_cat_ohe)"],"metadata":{"id":"DisCdo3NihMB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Ajusta o nome dos atributos como species_'nomecatX'\n","print(enc_ohe.get_feature_names_out(['species','gender']))"],"metadata":{"id":"0IS90EtPiwEv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para atributos categóricos binários, é possível remover uma das colunas após a codificação."],"metadata":{"id":"RCtMXZnhjmce"}},{"cell_type":"code","source":["enc_ohe = OneHotEncoder(sparse=False,drop='if_binary')\n","\n","X_train_cat_ohe = enc_ohe.fit_transform(X_train_cat)\n","X_test_cat_ohe = enc_ohe.transform(X_test_cat)"],"metadata":{"id":"B2-IwSuMjIsu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_test_cat_ohe)"],"metadata":{"id":"8wRe5DGMjQqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(enc_ohe.get_feature_names_out(['species','gender']))"],"metadata":{"id":"RamcuL8bjQjq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Em alguns domínios, o atributo categórico pode ter muitas categorias possíveis, aumentando muito a dimensionalidade do problema após a codificação. Nestes casos, pode ser útil mapear para novos atributos apenas as categorias mais frequentes, seja determinando um valor fixo de número de categorias (max_categories) ou determinando uma frequência mínima nos dados (min_frequency). As categorias infrequentes serão agregadas em uma única coluna após o processo de codificação. **(Esta opção não está disponível na versão '0.24.1' do scikit-learn,instalada por padrão no Google Colab)**"],"metadata":{"id":"uOcZqpsyj3-W"}},{"cell_type":"markdown","source":["\n","\n","##**Predição de 'churn' de clientes de serviço de telecomunicação**\n","\n","Tendo demonstrado o funcionamento básico de estratégias de preparação de dados, vamos aplicá-las a um conjunto de dados mais interessante.\n","\n","Os dados que utilizaremos neste notebook se referem a uma empresa de telecomunicações fictícia que forneceu serviços de telefone residencial e Internet para clientes na Califórnia. O conjunto de dados possui informações sobre os serviços para os quais cada cliente se inscreveu (telefone, várias linhas, internet, segurança online, backup online, proteção de dispositivos, suporte técnico e streaming de TV e filmes), informações da conta do cliente (há quanto tempo eles são clientes, contrato, forma de pagamento, cobrança sem papel, cobranças mensais e cobranças totais) e informações demográficas sobre os clientes (sexo, faixa etária, se eles têm parceiros e dependentes). Há, ainda, uma coluna chamada 'churn', que indica os clientes que desistiram do contrato do serviço no último mês. O objetivo da tarefa preditiva é identificar o churn (i.e., saída) de clientes a partir das informações coletadas. Os dados podem ser acessados neste [link](https://www.kaggle.com/datasets/blastchar/telco-customer-churn).\n"],"metadata":{"id":"DbJNQfaM4ERT"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"jiutqCcGYYM0"}},{"cell_type":"markdown","source":["###Carregando e inspecionando os dados\n","\n","Primeiramente, vamos carregar algumas bibliotecas importantes do Python e os dados a serem utilizados neste estudo. Os dados são disponibilizados através de um link, que também pode ser diretamente acessado pelos alunos."],"metadata":{"id":"dbr-VwMq6OPG"}},{"cell_type":"code","source":["## Bibliotecas para treinamento/avaliação de modelos\n","from sklearn.model_selection import RepeatedKFold, train_test_split, cross_validate, cross_val_score, GridSearchCV\n","from sklearn import metrics\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","sns.set()"],"metadata":{"id":"2lyNg5Sluhbg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install missingno ## instalando biblioteca para visualizar valores faltantes"],"metadata":{"id":"rf_FwP-d5pty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import missingno as msno"],"metadata":{"id":"Vg8iB83m5w3G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Carregando os dados\n","df = pd.read_csv(\"https://drive.google.com/uc?export=view&id=10VrzI8mA2wPvkNIDaLzv-IglPU-Febtf\",na_values=[\"NA\"])\n","df  "],"metadata":{"id":"bPcnFsAB9kv0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Características gerais do dataset\n","print(\"O conjunto de dados possui {} linhas e {} colunas\".format(df.shape[0], df.shape[1]))"],"metadata":{"id":"4BWj00iDltyH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Existem dados duplicados?\n","df.drop_duplicates(keep='last').shape"],"metadata":{"id":"lFbe4X4cPwNf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A coluna *'Churn'* contém a classificação de cada instância. Vamos avaliar a distribuição de classes do problema."],"metadata":{"id":"UqoChZTpAEMu"}},{"cell_type":"code","source":["## Distribuição do atributo alvo\n","plt.hist(df['Churn'])\n","plt.title(\"Distribuição do atributo alvo\")\n","plt.show()"],"metadata":{"id":"h0NT9TCiAeLX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Quais os tipos de atributos nos dados? Os dados possuem valores faltantes?\n","Podemos responder estas perguntas utilizando o método .info() para um dataframe."],"metadata":{"id":"ap9Axa7H1fpD"}},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"fwVKHnB1SdOM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A biblioteca missingno permite visualizar os dados faltantes como uma matriz. Outras opções de visualização estão disponíveis e podem ser consultadas na documentação da biblioteca."],"metadata":{"id":"rqyBrvdS1tFC"}},{"cell_type":"code","source":["msno.matrix(df)"],"metadata":{"id":"9aSnCISJ53EE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["O atributo customerID é único para cada instância e no geral não possui valor preditivo. Iremos removê-lo da análise."],"metadata":{"id":"J_EFTXXhkfXC"}},{"cell_type":"code","source":["customer_ids = df['customerID']\n","df = df.drop(['customerID'], axis=1)"],"metadata":{"id":"QC5YOq8Jkeq2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##remove as duplicatas\n","df=df.drop_duplicates(keep='last')"],"metadata":{"id":"lfnumOq4pdvG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como atributos categóricos e numéricos podem demandar pré-processamentos diferentes, vamos separar os respectivos \"nomes\" em dois vetores distintos, facilitando a manipulação dos dados posteriormente."],"metadata":{"id":"UoOevD6R14tT"}},{"cell_type":"code","source":["## Separa os atributos em vetores, de acordo com o tipo de dado (categórico ou numérico)\n","cat_columns=list(df.drop(['Churn'], axis=1).select_dtypes(include=[\"object\"]).columns)\n","print(cat_columns)\n","\n","num_columns=list(df.select_dtypes(include=[\"int64\", \"float64\"]).columns)\n","print(num_columns)"],"metadata":{"id":"XEkRHdAqSS4Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["É sempre importante inspecionar os dados. Em relação a atributos categóricos, podemos inspecionar os valores que cada um pode assumir, quantas instâncias temos por valor de atributo, distribuição destes entre classes (não inspecionado nas células a seguir), etc. Em relação a atributos numéricos, é importante observar a distribuição de valores e se há ocorrência de outliers. O boxplot, por padrão, mostra outliers de acordo com o método do IQR (valores que estão 1.5*IQR acima do terceiro quartil ou abaixo do primeiro quartil)."],"metadata":{"id":"RhmuEpuh2J8f"}},{"cell_type":"code","source":["def dist_plot(df,columns,type='boxplot'):\n","    plt.figure(figsize=(16, 5))\n","    for indx, var  in enumerate(columns):\n","        plt.subplot(1, 3, indx+1)\n","        if (type=='boxplot'):\n","          g = sns.boxplot(x=var, data=df,showfliers=True)\n","        else: \n","          if (type=='histogram'):\n","            g = sns.histplot(x=var, data=df)\n","    plt.tight_layout()\n","\n","def count_plot(df,columns):\n","    plt.figure(figsize=(20, 12))\n","    for indx, var  in enumerate(columns):\n","        plt.subplot(6, 3, indx+1)\n","        g = sns.countplot(x=var, data=df)\n","    plt.tight_layout()"],"metadata":{"id":"45aavabCVANv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count_plot(df,cat_columns)"],"metadata":{"id":"YJ0cj1hbmIUo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Alguma variável categórica pode ser interpretada como categórica ordinal, ou todas são categóricas nominais? Esta distinção é importante para avaliar o tipo de codificação a ser aplicada na transformação dos dados."],"metadata":{"id":"hJwUFIPqnac1"}},{"cell_type":"code","source":["dist_plot(df,num_columns)#,type=\"histogram\")"],"metadata":{"id":"DcfYrcxOlASp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["O boxplot indica a existência de outliers (univariados) no conjunto de dados?"],"metadata":{"id":"ZfNFgFKym9dG"}},{"cell_type":"markdown","source":["\n","---\n","\n","\n","### Criando conjuntos de treino e teste para avaliação de modelos\n","\n","\n","Antes de iniciar o treinamento do modelo, lembre-se que é recomendado sempre reservar uma porção dos dados para teste, a qual somente será utilizada para avaliação do modelo final (após todo o processo de treinamento e otimização de hiperparâmetros).\n","\n","Vamos fazer esta divisão, separando 20% para teste. Entretanto, primeiro precisamos dividir os dados entre atributos (X) e classe (y). Também iremos codificar as classes Yes/No para 1/0.\n","\n"],"metadata":{"id":"63nQ-E0jVQzW"}},{"cell_type":"code","source":["## Separa o dataset em duas variáveis: os atributos/entradas (X) e a classe/saída (y)\n","X = df.drop(['Churn'], axis=1)\n","y = df['Churn'].values"],"metadata":{"id":"pd1ZSQRnXQq3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Faremos o mapeamento das classes Yes/No para 1/0. Por padrão, as funções de avaliação assumem que a classe 1 é a positiva/de interesse."],"metadata":{"id":"AfWwwt-QiB16"}},{"cell_type":"code","source":["## substitui No' por 0, 'Yes' por 1\n","y = np.array([0 if y=='No' else 1 for y in y]) "],"metadata":{"id":"vXM10ZIZjZzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Faz a divisão entre treino (80%) e teste (20%).\n","## O conjunto de treino representa os dados que serão usados\n","## ao longo do desenvolvimento do modelo\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,stratify=y,random_state=42) "],"metadata":{"id":"eZbsoLfsXkoZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","## Lidando com valores faltantes\n","\n","Na análise exploratória pudemos perceber que temos alguns valores faltantes para duas variáveis, 'tenure' e 'gender'. Precisamos definir como tratar estes valores faltantes: removemos a instância, imputamos com um valor constante, imputamos com uma estatística, imputamos usando uma estimador multivariado (por exemplo, um KNN)? Abaixo vamos ver algumas estratégias possíveis. \n","\n","As células abaixo sumarizam os valores faltantes após divisão de treino/teste."],"metadata":{"id":"ZkOKOBleLW0b"}},{"cell_type":"code","source":["msno.matrix(pd.DataFrame(X_train))"],"metadata":{"id":"qUx7QZIn6Ajz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(X_train).info()"],"metadata":{"id":"pj-bYpSVwoHG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(X_test).info()"],"metadata":{"id":"HUzs7PPywo7o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Uma opção seria simplesmente remover as instâncias com valores faltantes. Em domínios com muitos dados e pouca proporção de valores faltantes, esta remoção normalmente não terá implicações negativas. A célula abaixo mostra como isso poderia ser feito, salvando os novos dados em 'df2'"],"metadata":{"id":"ZpwQ4Gkr6WEU"}},{"cell_type":"code","source":["## para remover as instâncias com valores faltantes \n","## (neste caso pode ser aplicado a todo o conjunto de dados)\n","df2 = df.dropna(inplace=False)\n","print(\"O conjunto de dados após remoção de valores faltantes possui {} linhas e {} colunas\".format(df2.shape[0], df2.shape[1]))"],"metadata":{"id":"rhhDVgNDq3rB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Não existem muitos valores faltantes. Ainda assim, os algoritmos em Python demandam que os valores faltantes sejam tratados de alguma forma. Uma alternativa à remoção das instâncias com valores faltantes é explorar estratégias de imputação de valores para substituí-los por algum outro valor simbólico dentro da distribuição original dos dados. Como temos atributos numéricos e categóricos, o valor a ser substituído (ou a estratégia para definir o valor) depende do tipo de dado. \n","\n","Os 'imputers' definidos abaixo lidam com esta especificidade. Vamos explorar o uso do SimpleImputer, capaz de lidar com valores numéricos ou categóricos.\n","\n","Perceba que o método ColumnTransformer permite aplicar diferentes tipos de pré-processamento a diferentes grupos de atributos (colunas)."],"metadata":{"id":"q8kr6qb1t0dI"}},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","\n","## para o atributo numérico podemos usar a mediana\n","imputer_Median = SimpleImputer(strategy='median')\n","\n","## para o atributo categórico, podemos usar a moda (valor mais frequente)\n","imputer_Mode = SimpleImputer(strategy='most_frequent')\n","\n","## Cada \"imputer\" deve ser ajustado (.fit())ao seu respectivo tipo de dado \n","## Uma forma prática de fazer isto no Python é usar o ColumnTransformer\n","## O ColumnTransformer permite aplicar diferentes transformações a colunas específicas\n","\n","col_imputer = ColumnTransformer(transformers=[('imputer_median', imputer_Median, num_columns),\n","                                               ('imputer_mode',  imputer_Mode, cat_columns)],\n","                                               remainder='passthrough')\n","## 'ajustamos' os parâmetros de imputação a partir dos dados de treinamento\n","col_imputer.fit(X_train)\n","\n","## aplicamos para imputação no treino e no teste\n","X_train_imp1 = col_imputer.transform(X_train)\n","X_test_imp1 = col_imputer.transform(X_test)"],"metadata":{"id":"RHYYXPrFpVnt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## De acordo com a documentação:\n","# The order of the columns in the transformed feature matrix follows the order of \n","# how the columns are specified in the transformers list. \n","\n","print(col_imputer.transformers_[0][2])\n","print(col_imputer.transformers_[1][2])"],"metadata":{"id":"gUX4R6XNBOvl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ajusta nome das columas e mostra dataframe\n","columns = np.append(num_columns,cat_columns)\n","display(pd.DataFrame(X_train_imp1, columns=columns))"],"metadata":{"id":"1bHHJnJveqqb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Verificando o resultado através da inspeção dos dados e contagem de valores não-nulos"],"metadata":{"id":"chKq8Ypsw1ud"}},{"cell_type":"code","source":["msno.matrix(pd.DataFrame(X_train_imp1))"],"metadata":{"id":"l1UsgBH16cMY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(X_train_imp1).info()"],"metadata":{"id":"uTD6aQwZwgUH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(X_test_imp1).info()"],"metadata":{"id":"1YJPOU0Rwxn_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Outra possibilidade é usar o KNNImputer, entretanto, o mesmo possui a limitação de esperar como entrada apenas valores numéricos. Para valores categóricos, os mesmos precisariam ser convertidos em numéricos através de alguma estratégia de codificação. Como os dados ainda não foram transformados, usaremos o ColumnTransformer para exemplificar a aplicação do KNNImputer apenas nos atributos numéricos."],"metadata":{"id":"1S-SHR1T7Bk9"}},{"cell_type":"code","source":["from sklearn.impute import KNNImputer\n","\n","## para o atributo numérico podemos usar o KNNImputer\n","imputer_KNN = KNNImputer(n_neighbors=3, weights=\"uniform\")\n","\n","## para o atributo categórico, podemos usar a moda com SimpleImputer\n","imputer_Mode = SimpleImputer(strategy='most_frequent')\n","\n","## Cada \"imputer\" deve ser ajustado (.fit()) ao seu respectivo tipo de dado \n","## Uma forma prática de fazer isto no Python é usar o ColumnTransformer\n","## O ColumnTransformer permite aplicar diferentes transformações a colunas específicas\n","\n","col_imputer = ColumnTransformer(transformers=[('imputer_KNN', imputer_KNN, num_columns),\n","                                               ('imputer_mode',  imputer_Mode, cat_columns)])\n","## 'ajustamos' os parâmetros de imputação a partir dos dados de treinamento\n","col_imputer.fit(X_train)\n","\n","## aplicamos para imputação no treino e no teste\n","X_train_imp2 = col_imputer.transform(X_train)\n","X_test_imp2 = col_imputer.transform(X_test)"],"metadata":{"id":"szhdvIZfw_OW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ajusta nome das columas e mostra dataframe\n","columns = np.append(num_columns,cat_columns)\n","display(pd.DataFrame(X_train_imp2, columns=columns))"],"metadata":{"id":"yUQ-ktUxgg7K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","## Realizando a conversão de atributos categóricos para numéricos\n","\n","Os algoritmos de aprendizado supervisionado implementados em Python, de forma geral, não lidam bem com valores categóricos que estão representados como strings ou caracteres. Existem ainda algoritmos que por padrão, não recebem valores categóricos como entrada (por exemplo, regressão logística e redes neurais). Assim, é preciso codificar os valores categóricos para numéricos. A maioria dos atributos categóricos no conjunto de dados analisado são do tipo nominal: para atributos nominal, é indicado o uso de OneHotEncoder(). No caso de atributos ordinais, é indicado o uso de OrdinalEncoder().\n"],"metadata":{"id":"SknGaWAAC5bE"}},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","from sklearn.pipeline import Pipeline\n","\n","num_pipeline = Pipeline([\n","    ('imputer', SimpleImputer(strategy='median'))])\n","\n","cat_pipeline = Pipeline([\n","    ('imputer', SimpleImputer(strategy='most_frequent')),\n","    ('encoder', OneHotEncoder(drop='if_binary', sparse=False))])\n","\n","data_pipeline = ColumnTransformer([\n","    ('numerical', num_pipeline, num_columns),\n","    ('categorical', cat_pipeline, cat_columns),\n","    \n","])\n","\n","X_train_preproc1 = data_pipeline.fit_transform(X_train)\n","X_test_preproc1 = data_pipeline.transform(X_test)"],"metadata":{"id":"ZUTZRNo-pAiQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ajusta nome das columas e mostra dataframe após pré-processamento\n","columns = np.append(num_columns,data_pipeline.named_transformers_['categorical']['encoder'].get_feature_names_out(cat_columns))\n","display(pd.DataFrame(X_train_preproc1, columns=columns))"],"metadata":{"id":"d9TSQ8Gjg4Jx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","## Normalizando os dados\n","\n","Podemos observar que mesmo após a imputação de valores faltantes e conversão de atributos categóricos para numéricos, o conjunto de dados ainda apresenta uma característica que pode ser crítica para alguns algoritmos de aprendizado: os atributos possuem diferenças de escalas. Assim, é preciso normalizar os valores.\n","\n","Uma vez que todos os atributos se tornaram numéricos após a execução do pipeline anterior, podemos aplicar a normalização a todos o atributos, independente do ColumnTransformer. "],"metadata":{"id":"-HADnsnKVoUY"}},{"cell_type":"code","source":["# from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","\n","num_pipeline = Pipeline([\n","    ('imputer', SimpleImputer(strategy='median'))])\n","\n","cat_pipeline = Pipeline([\n","                         ('imputer', SimpleImputer(strategy='most_frequent')),\n","                         ('encoder', OneHotEncoder(drop='if_binary', sparse=False))])\n","\n","data_pipeline = ColumnTransformer([\n","                                   ('numerical', num_pipeline, num_columns),\n","                                   ('categorical', cat_pipeline, cat_columns)])\n","\n","norm_pipe = Pipeline([\n","                 ('data_transform', data_pipeline),\n","                 ('data_normalize',MinMaxScaler())])\n","\n","\n","X_train_preproc2 = norm_pipe.fit_transform(X_train)\n","X_test_preproc2 = norm_pipe.transform(X_test)\n"],"metadata":{"id":"Op7SKfhGWh3X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ajusta nome das columas e mostra dataframe após pré-processamento\n","columns = np.append(num_columns,norm_pipe[0].named_transformers_['categorical']['encoder'].get_feature_names_out(cat_columns))\n","display(pd.DataFrame(X_train_preproc2, columns=columns))\n"],"metadata":{"id":"WRN5rug7j_WV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["É importante notar que para o caso de codificação de valores categóricos com One-Hot Encoder, todos os atributos já estão naturalmente no intervalo [0,1], ou seja, estariam normalizados de acordo com o método MinMaxScaler(). Desta forma, poderíamos ter optado colocar a chamada ao método MinMaxScaler() dentro do pipeline para valores numéricos (num_pipeline). Entretanto, no caso de usarmos o método OrdinalEncoder, ou ainda no caso de querermos aplicar a padronização dos dados (StandardScaler()), a normalização/padronização seria feita sobre todos os atributos conforme fizemos na célula acima."],"metadata":{"id":"GbZIm3-gknRO"}},{"cell_type":"markdown","source":["---\n","\n","\n","## Agregando ao *Pipeline* o treinamento de modelos\n","\n","Até o momento, nossos dados passaram pelos seguintes pré-processamentos:\n","\n","\n","*   Atributos **numéricos**: imputação de valores faltantes e normalização\n","*   Atributos **categóricos**: imputação de valores faltantes, one-hot encoding, e normalização.\n","\n","Ao concluir este pipeline, nossos dados de Treino e teste estão prontos para o treinamento de modelos. Abaixo vamos exemplificar como a etapa de treinamento do modelo pode ser incorporando ao pipeline, como o passo final da sequência de preparação e análise dos dados. \n","\n","Iremos repetir os códigos da célula anterior (na Seção 'Normalizando os dados') para tornar o código mais claro, embora todos os pipelines/dados salvos em variáveis possam ser reutilizados.*italicized text*\n"],"metadata":{"id":"z2gkBoZsXngt"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","num_pipeline = Pipeline([\n","    ('imputer', SimpleImputer(strategy='median'))])\n","\n","cat_pipeline = Pipeline([\n","                         ('imputer', SimpleImputer(strategy='most_frequent')),\n","                         ('encoder', OneHotEncoder(drop='if_binary', sparse=False))])\n","\n","data_pipeline = ColumnTransformer([\n","                                   ('numerical', num_pipeline, num_columns),\n","                                   ('categorical', cat_pipeline, cat_columns)])\n","\n","preproc_train_pipe = Pipeline([\n","                 ('data_transform', data_pipeline),\n","                 ('data_normalize',MinMaxScaler()),\n","                 ('model',LogisticRegression())])\n","\n","## para um pipeline que termina com um modelo a ser treinado, \n","## a chamada para \"ajuste\" aos dados de treinamento ocorre com .fit () e recebe os \n","## atributos (X) e a classe (y). Achamada para aplicação do pipeline aos dados de \n","##treino/teste ocorre com .predict()\n","\n","preproc_train_pipe.fit(X_train,y_train)\n","\n","y_train_pred = preproc_train_pipe.predict(X_train)\n","y_test_pred = preproc_train_pipe.predict(X_test)\n"],"metadata":{"id":"6_zFvz2vmUEt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, recall_score, precision_score,accuracy_score,f1_score,ConfusionMatrixDisplay ## para avaliação dos modelos\n","\n","cm = confusion_matrix(y_test, y_test_pred,labels=preproc_train_pipe[2].classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=preproc_train_pipe[2].classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()\n","\n","print('Acurácia: {}'.format(round(accuracy_score(y_test, y_test_pred),3)))\n","print('Recall: {}'.format(round(recall_score(y_test, y_test_pred,pos_label=1),3)))\n","print('Precisão: {}'.format(round(precision_score(y_test,y_test_pred,pos_label=1),3)))\n","print('F1-Score: {}'.format(round(f1_score(y_test,y_test_pred,pos_label=1),3)))"],"metadata":{"id":"LoaMBI1mn4l_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Uso de pipelines com GridSearch"],"metadata":{"id":"jIwyo7uptdm3"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","## Cria o pipeline semelhante ao exemplo anteri\n","preproc_knn_pipe = Pipeline([\n","                 ('data_transform', data_pipeline),\n","                 ('data_normalize',MinMaxScaler()),\n","                 ('model',KNeighborsClassifier())])\n","\n","## Cria uma grid de busca para o KNN\n","## Os hiperparâmetros de um pipeline são especificados como <step name>__<hyperparameter name>\n","param_grid = {'model__n_neighbors': range(1, 10)}\n","\n","## Instancia uma Grid Search com o pipeline (incluindo etapas de transformação de dados\n","## e treinamento do modelo) e 10-fold cross-validation\n","grid = GridSearchCV(preproc_knn_pipe, param_grid, cv=10,scoring='f1',refit=True)\n","\n","## Executa a grid Search\n","grid.fit(X_train, y_train)\n","print(grid.best_params_)\n","print(grid.score(X_test, y_test))"],"metadata":{"id":"enozjQhLo1iN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cm = confusion_matrix(y_test, grid.predict(X_test),labels=preproc_train_pipe[2].classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=preproc_train_pipe[2].classes_)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()"],"metadata":{"id":"nxHvzf-ZoBmR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Uso de pipelines com Cross-Validation no Spot-Checking"],"metadata":{"id":"QWWyj-ryth5C"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","preproc_train_pipe = Pipeline([\n","                 ('data_transform', data_pipeline),\n","                 ('data_normalize',MinMaxScaler()),\n","                 ('model',LogisticRegression())])\n","\n","clfs = []\n","clfs.append(LogisticRegression())\n","clfs.append(SVC(kernel='linear'))\n","clfs.append(SVC(kernel='rbf'))\n","clfs.append(GaussianNB())\n","clfs.append(KNeighborsClassifier(n_neighbors=3))\n","clfs.append(DecisionTreeClassifier())\n","clfs.append(RandomForestClassifier())\n","clfs.append(BaggingClassifier())\n","\n","for classifier in clfs:\n","    preproc_train_pipe.set_params(model = classifier)\n","    scores = cross_validate(preproc_train_pipe, X_train, y_train,scoring=['f1','recall','precision'])\n","    print('---------------------------------')\n","    print(str(classifier))\n","    print('-----------------------------------')\n","    for key, values in scores.items():\n","            print(key,' mean ', values.mean())\n","            print(key,' std ', values.std())"],"metadata":{"id":"c_HQaUtMsLLg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Uso de GridSearch para otimizar escolhas de pré-processamento"],"metadata":{"id":"D8tb3KL1tt30"}},{"cell_type":"markdown","source":["Nos exemplos anteriores com uso de Grid Search e k-fold cross-validation, o objetivo era otimizar os hiperparâmetros do algoritmo de aprendizado. Entretanto, também é possível utilizar este processo para otimizar as estratégias de pré-processamento de dados, visando obter melhor poder de generalização.\n","\n","No exemplo abaixo, iremos demonstrar como utilizar esta estratégia para definir qual o melhor método de normalização dos dados:"],"metadata":{"id":"Zjs6PCYst0Ff"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler\n","\n","# declaramos o pipeline a ser utilizado, dando nomes a cada etapa de forma explícita\n","\n","num_pipeline = Pipeline([\n","    ('imputer', SimpleImputer(strategy='median'))])\n","\n","cat_pipeline = Pipeline([\n","                         ('imputer', SimpleImputer(strategy='most_frequent')),\n","                         ('encoder', OneHotEncoder(drop='if_binary', sparse=False))])\n","\n","data_pipeline = ColumnTransformer([\n","                                   ('numerical', num_pipeline, num_columns),\n","                                   ('categorical', cat_pipeline, cat_columns)])\n","\n","preproc_train_pipe = Pipeline([\n","                 ('data_transform', data_pipeline),\n","                 ('scaler',MinMaxScaler()),\n","                 ('knn',KNeighborsClassifier())])\n","\n","## A etapa/passo de normalização é denonimado 'scaler'. Podemos atribuir diferentes\n","## estratégias para este passo em uma grid de hiperparâmetros (incluindo a opção 'passthrough',\n","## que ignora a etapa). Da mesma forma, iremos definir valores de k-vizinhos mais próximos para avaliar.\n","\n","param_grid = {'scaler': [MinMaxScaler(), StandardScaler(), RobustScaler(),'passthrough'],\n","              # we named the second step knn, so we have to use that name here\n","              'knn__n_neighbors': range(1, 20)}\n","\n","## Instancia e executa o GridSearch\n","grid = GridSearchCV(preproc_train_pipe, param_grid, cv=10,scoring='f1',refit=True)\n","grid.fit(X_train, y_train)\n","print(grid.best_params_)\n","print(grid.score(X_test, y_test))"],"metadata":{"id":"C6wlzplmuMxo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cm = confusion_matrix(y_test, grid.predict(X_test))\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n","plt.grid(False)\n","plt.show()"],"metadata":{"id":"NWj6n75SvQvw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Este notebook explorou diferentes estratégias para iniciarmos no pré-processamento de dados (imputação de valores, discretização de dados, codificação de dados e normalização de dados) e demonstrou o uso de Pipelines em Python para criar pipelines de pré-processamento e treinamento facilmente reutilizáveis - incluindo o reuso dentre de loops de GridSearch com validação cruzada.\n","\n","A partir dos conhecimentos obtidos nessa aula, você já é capaz de implementar algumas etapas iniciais de pré-processamento de dados no ciclo de desenvolvimento de modelos de Aprendizado de Máquina!"],"metadata":{"id":"SRX0yJjQyNtr"}},{"cell_type":"markdown","source":["## Sua Vez!\n","\n","Esta atividade prática já será um \"aquecimento\" para o projeto final da disciplina e visa permitir que os alunos explorem, de forma autônoma, o uso de métodos de pré-processamento e pipelines no desenvolvimento de modelos preditivos em Aprendizado de Máquina.\n","\n","Os alunos devem:\n","\n","*   Se organizar em grupos de **até 3 alunos** (interessante que já seja o grupo para realização do projeto final da disciplina).\n","*   Escolher um conjunto de dados que **não está pronto** para análise por algoritmos de aprendizado supervisionado, apresentando a **necessidade de limpeza e/ou transformação de dados**. Idealmente, selecionar um conjunto de dados que contenha tipos mistos de atributos e que seja de interesse do grupo para realização do projeto final\n","*   Implementar um pipeline (recomendado o uso de `Pipeline` no scikit-learn) para realizar o pré-processamento de dados utilizando os métodos vistos nesta aula: imputação de valores, codificação de dados categóricos, discretização de dados numéricos, normalização de dados. Eventualmente nem todos serão necessários nos dados, mas sugere-se tentar usar ao menos dois destes. A imputação de valores pode ser utilizada tanto para corrigir valores faltantes como ruídos/outliers.\n","*   Incluir no pipeline o treinamento de modelos de aprendizado de máquina. Os grupos podem optar por realizar um spot-checking de algoritmos e/ou selecionar um algoritmo específico e realizar a otimização dos seus hiperparâmetros, conforme exemplos deste notebook.\n","*    Executar o pipeline para os dados selecionados, aplicando as etapas de pré-processamento e treinamento de modelos definidas pelo grupo. \n","*    Avaliar o desempenho do modelo final com os dados de teste (evitem data leakage!), reportando matriz de confusão e métrica(s) de desempenho(s) selecionada(s) pelo grupo.\n","\n","Os grupos devem submeter no Moodle o seu notebook do Google Colab exportado em .pdf e .ypnb, devidamente identificado com os nomes dos integrantes do grupo. Sugere-se que, se possível, os grupos incluam no notebook a ser enviado o link para o Google Colab na nuvem, com saídas salvas, a fim de que a professora e monitores(as) possam acessá-lo em caso de dúvidas na avaliação.\n"],"metadata":{"id":"AbSTMZPJy36O"}}]}